#!/usr/bin/env python3
"""
ğŸ¤– ìë™ ìš´ë™ ê°ì§€ + ê°ë„ ë¶„ì„ í†µí•© ì‹œìŠ¤í…œ
1ë‹¨ê³„: AIê°€ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€
2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ê°ë„ ë¶„ì„ + ì´ˆë¡/ë¹¨ê°• í™”ë©´ í‘œì‹œ
"""

import cv2
import numpy as np
import mediapipe as mp
import time
import argparse
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import deque
import json
from datetime import datetime

class AutoExerciseAnalyzer:
    """ìë™ ìš´ë™ ê°ì§€ + ê°ë„ ë¶„ì„ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # MediaPipe ì´ˆê¸°í™”
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
        self.exercise_classifier = None
        self.model_loaded = False
        self.load_exercise_model()
        
        # Enhanced ê°ë„ ê¸°ì¤€
        self.exercise_thresholds = {
            'squat': {
                'left_knee': {'points': [23, 25, 27], 'range': (55, 140), 'weight': 1.1},
                'right_knee': {'points': [24, 26, 28], 'range': (55, 140), 'weight': 1.1},
                'left_hip': {'points': [11, 23, 25], 'range': (55, 140), 'weight': 0.9},
                'right_hip': {'points': [12, 24, 26], 'range': (55, 140), 'weight': 0.9},
                'back_straight': {'points': [11, 23, 25], 'range': (110, 170), 'weight': 1.1},
            },
            'push_up': {
                'left_elbow': {'points': [11, 13, 15], 'range': (40, 160), 'weight': 1.0},
                'right_elbow': {'points': [12, 14, 16], 'range': (40, 160), 'weight': 1.0},
                'body_line': {'points': [11, 23, 25], 'range': (140, 180), 'weight': 1.2},
                'leg_straight': {'points': [23, 25, 27], 'range': (140, 180), 'weight': 0.8},
            },
            'deadlift': {
                'left_knee': {'points': [23, 25, 27], 'range': (80, 140), 'weight': 0.6},
                'right_knee': {'points': [24, 26, 28], 'range': (80, 140), 'weight': 0.6},
                'hip_hinge': {'points': [11, 23, 25], 'range': (80, 180), 'weight': 0.7},
                'back_straight': {'points': [11, 23, 12], 'range': (120, 180), 'weight': 1.0},
            },
            'bench_press': {
                'left_elbow': {'points': [11, 13, 15], 'range': (50, 145), 'weight': 1.1},
                'right_elbow': {'points': [12, 14, 16], 'range': (50, 145), 'weight': 1.1},
                'left_shoulder': {'points': [13, 11, 23], 'range': (50, 150), 'weight': 0.9},
                'right_shoulder': {'points': [14, 12, 24], 'range': (50, 150), 'weight': 0.9},
            },
            'lunge': {
                'front_knee': {'points': [23, 25, 27], 'range': (70, 120), 'weight': 1.2},
                'back_knee': {'points': [24, 26, 28], 'range': (120, 180), 'weight': 1.0},
                'front_hip': {'points': [11, 23, 25], 'range': (70, 120), 'weight': 0.8},
                'torso_upright': {'points': [11, 23, 25], 'range': (100, 180), 'weight': 1.2},
            }
        }
        
        # Enhanced ë¶„ë¥˜ ì„ê³„ê°’
        self.classification_thresholds = {
            'squat': 0.5,
            'push_up': 0.7,
            'deadlift': 0.8,  # ì™„í™”
            'bench_press': 0.5,
            'lunge': 0.6,
        }
        
        # ìš´ë™ ì´ëª¨ì§€
        self.exercise_emojis = {
            'squat': 'ğŸ‹ï¸â€â™€ï¸',
            'push_up': 'ğŸ’ª',
            'deadlift': 'ğŸ‹ï¸â€â™‚ï¸',
            'bench_press': 'ğŸ”¥',
            'lunge': 'ğŸš€'
        }
        
        # ìƒíƒœ ê´€ë¦¬
        self.current_exercise = "detecting..."
        self.current_pose_quality = "unknown"
        self.exercise_confidence = 0.0
        self.pose_confidence = 0.0
        
        # ì•ˆì •í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬
        self.exercise_history = deque(maxlen=10)
        self.pose_history = deque(maxlen=5)
        
        # í†µê³„
        self.stats = {'good': 0, 'bad': 0, 'frames': 0}
        
        # í™”ë©´ ìƒíƒœ (ë¶€ë“œëŸ¬ìš´ ì „í™˜)
        self.screen_color = (128, 128, 128)  # ê¸°ë³¸ íšŒìƒ‰
        self.target_color = (128, 128, 128)
        self.color_transition_speed = 0.1
        
        # íƒ€ì´ë°
        self.last_classification_time = 0
        self.classification_interval = 2.0  # 2ì´ˆë§ˆë‹¤ ìš´ë™ ë¶„ë¥˜
    
    def load_exercise_model(self):
        """AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ"""
        model_path = "models/exercise_classifier.pkl"
        try:
            if os.path.exists(model_path):
                from exercise_classifier import ExerciseClassificationModel
                self.exercise_classifier = ExerciseClassificationModel()
                self.model_loaded = self.exercise_classifier.load_model(model_path)
                if self.model_loaded:
                    print("âœ… AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            else:
                print("âš ï¸ AI ëª¨ë¸ ì—†ìŒ - ìˆ˜ë™ ìš´ë™ ì„ íƒ ëª¨ë“œ")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.model_loaded = False
    
    def calculate_angle(self, p1: Tuple[float, float], p2: Tuple[float, float], p3: Tuple[float, float]) -> float:
        """ê°ë„ ê³„ì‚°"""
        try:
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]], dtype=np.float64)
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]], dtype=np.float64)
            
            v1_mag = np.linalg.norm(v1)
            v2_mag = np.linalg.norm(v2)
            
            if v1_mag < 1e-6 or v2_mag < 1e-6:
                return 180.0
            
            cos_angle = np.dot(v1, v2) / (v1_mag * v2_mag)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.degrees(np.arccos(cos_angle))
            
            return float(angle)
        except:
            return 180.0
    
    def classify_exercise(self, frame: np.ndarray) -> Tuple[str, float]:
        """ğŸ¤– 1ë‹¨ê³„: AIë¡œ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€"""
        current_time = time.time()
        
        # ë¶„ë¥˜ ì£¼ê¸° ì œì–´ (2ì´ˆë§ˆë‹¤)
        if current_time - self.last_classification_time < self.classification_interval:
            return self.current_exercise, self.exercise_confidence
        
        if not self.model_loaded:
            return "manual_mode", 0.0
        
        try:
            # ì„ì‹œ ì´ë¯¸ì§€ ì €ì¥
            temp_path = "temp_frame.jpg"
            cv2.imwrite(temp_path, frame)
            
            # AI ìš´ë™ ë¶„ë¥˜
            exercise, confidence = self.exercise_classifier.predict(temp_path)
            
            # íˆìŠ¤í† ë¦¬ ì•ˆì •í™”
            self.exercise_history.append((exercise, confidence))
            
            if len(self.exercise_history) >= 3:
                # ìµœê·¼ 3ê°œ ê²°ê³¼ì˜ í•©ì˜
                recent = list(self.exercise_history)[-3:]
                high_conf_predictions = [(ex, conf) for ex, conf in recent if conf > 0.6]
                
                if high_conf_predictions:
                    from collections import Counter
                    exercises = [ex for ex, conf in high_conf_predictions]
                    most_common = Counter(exercises).most_common(1)[0]
                    
                    if most_common[1] >= 2:  # 2ë²ˆ ì´ìƒ ê°ì§€
                        new_exercise = most_common[0]
                        if new_exercise != self.current_exercise:
                            self.current_exercise = new_exercise
                            self.exercise_confidence = confidence
                            emoji = self.exercise_emojis.get(new_exercise, 'ğŸ‹ï¸')
                            print(f"ğŸ¤– AI ê°ì§€: {emoji} {new_exercise.upper()} (ì‹ ë¢°ë„: {confidence:.1%})")
            
            self.last_classification_time = current_time
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            return self.current_exercise, self.exercise_confidence
            
        except Exception as e:
            print(f"ìš´ë™ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return self.current_exercise, self.exercise_confidence
    
    def analyze_pose_angles(self, landmarks, exercise: str) -> Dict:
        """ğŸ¯ 2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ê°ë„ ë¶„ì„"""
        if exercise not in self.exercise_thresholds:
            return {'valid': False, 'error': f'ì§€ì›ë˜ì§€ ì•ŠëŠ” ìš´ë™: {exercise}'}
        
        thresholds = self.exercise_thresholds[exercise]
        angles = {}
        violations = []
        weighted_violation_score = 0.0
        total_weight = 0.0
        
        for joint_name, config in thresholds.items():
            try:
                p1_idx, p2_idx, p3_idx = config['points']
                min_angle, max_angle = config['range']
                weight = config['weight']
                
                # ê°€ì‹œì„± í™•ì¸
                if (landmarks[p1_idx].visibility < 0.25 or 
                    landmarks[p2_idx].visibility < 0.25 or 
                    landmarks[p3_idx].visibility < 0.25):
                    continue
                
                p1 = (landmarks[p1_idx].x, landmarks[p1_idx].y)
                p2 = (landmarks[p2_idx].x, landmarks[p2_idx].y)
                p3 = (landmarks[p3_idx].x, landmarks[p3_idx].y)
                
                angle = self.calculate_angle(p1, p2, p3)
                angles[joint_name] = {
                    'value': angle,
                    'range': (min_angle, max_angle),
                    'weight': weight,
                    'in_range': min_angle <= angle <= max_angle
                }
                
                total_weight += weight
                
                if not (min_angle <= angle <= max_angle):
                    violations.append({
                        'joint': joint_name,
                        'angle': angle,
                        'expected_range': (min_angle, max_angle),
                        'weight': weight
                    })
                    weighted_violation_score += weight
                    
            except Exception as e:
                continue
        
        # Enhanced ë¶„ë¥˜
        violation_ratio = weighted_violation_score / max(total_weight, 1.0)
        classification_threshold = self.classification_thresholds.get(exercise, 0.6)
        is_good = violation_ratio < classification_threshold
        
        return {
            'valid': True,
            'classification': 'good' if is_good else 'bad',
            'confidence': 1.0 - violation_ratio,
            'angles': angles,
            'violations': violations,
            'violation_ratio': violation_ratio,
            'threshold': classification_threshold
        }
    
    def update_screen_color(self, pose_quality: str):
        """ğŸŒˆ ì´ˆë¡/ë¹¨ê°• í™”ë©´ ìƒ‰ìƒ ì—…ë°ì´íŠ¸"""
        if pose_quality == 'good':
            self.target_color = (0, 255, 0)      # ì´ˆë¡ìƒ‰
        elif pose_quality == 'bad':
            self.target_color = (0, 0, 255)      # ë¹¨ê°„ìƒ‰
        elif pose_quality == 'detecting':
            self.target_color = (255, 255, 0)    # ë…¸ë€ìƒ‰
        else:
            self.target_color = (128, 128, 128)  # íšŒìƒ‰
        
        # ë¶€ë“œëŸ¬ìš´ ìƒ‰ìƒ ì „í™˜
        for i in range(3):
            current = self.screen_color[i]
            target = self.target_color[i]
            diff = target - current
            self.screen_color = tuple(
                int(current + diff * self.color_transition_speed) if j == i 
                else self.screen_color[j] for j in range(3)
            )
    
    def draw_analysis_overlay(self, frame: np.ndarray, exercise: str, pose_result: Dict) -> np.ndarray:
        """ë¶„ì„ ê²°ê³¼ í™”ë©´ ì˜¤ë²„ë ˆì´"""
        height, width = frame.shape[:2]
        
        # ğŸŒˆ ì „ì²´ í™”ë©´ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´
        if pose_result.get('valid', False):
            pose_quality = pose_result['classification']
            self.update_screen_color(pose_quality)
            
            # íˆ¬ëª…í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´
            overlay = frame.copy()
            cv2.rectangle(overlay, (0, 0), (width, height), self.screen_color, -1)
            cv2.addWeighted(overlay, 0.15, frame, 0.85, 0, frame)
        
        # ğŸ¯ ë‘êº¼ìš´ í…Œë‘ë¦¬
        border_thickness = 25
        cv2.rectangle(frame, (0, 0), (width, height), self.screen_color, border_thickness)
        
        # ğŸ“Š ìƒë‹¨ ì •ë³´ íŒ¨ë„
        panel_height = 150
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, 0), (width, panel_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.8, frame, 0.2, 0, frame)
        
        # ğŸ¤– 1ë‹¨ê³„: AI ê°ì§€ ê²°ê³¼
        if exercise != "detecting..." and exercise != "manual_mode":
            emoji = self.exercise_emojis.get(exercise, 'ğŸ‹ï¸')
            exercise_text = f"AI ê°ì§€: {emoji} {exercise.upper().replace('_', ' ')}"
            cv2.putText(frame, exercise_text, (30, 45), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
            
            confidence_text = f"ë¶„ë¥˜ ì‹ ë¢°ë„: {self.exercise_confidence:.0%}"
            cv2.putText(frame, confidence_text, (30, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 2)
        elif exercise == "detecting...":
            cv2.putText(frame, "ğŸ¤– AIê°€ ìš´ë™ì„ ê°ì§€í•˜ëŠ” ì¤‘...", (30, 45), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 0), 2)
        else:
            cv2.putText(frame, "ìˆ˜ë™ ëª¨ë“œ - Cí‚¤ë¡œ ìš´ë™ ì„ íƒ", (30, 45), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)
        
        # ğŸ¯ 2ë‹¨ê³„: ìì„¸ ë¶„ì„ ê²°ê³¼
        if pose_result.get('valid', False):
            pose_quality = pose_result['classification']
            pose_confidence = pose_result['confidence']
            
            # ì¤‘ì•™ ìƒíƒœ ë©”ì‹œì§€
            if pose_quality == 'good':
                status_text = "ì™„ë²½í•œ ìì„¸! ğŸ‘"
                status_color = (0, 255, 0)
            else:
                status_text = "ìì„¸ êµì • í•„ìš” âš ï¸"
                status_color = (0, 0, 255)
            
            status_size = cv2.getTextSize(status_text, cv2.FONT_HERSHEY_SIMPLEX, 1.8, 4)[0]
            status_x = (width - status_size[0]) // 2
            status_y = height // 2 - 50
            
            # ìƒíƒœ ë°°ê²½
            cv2.rectangle(frame, (status_x - 30, status_y - 50), 
                         (status_x + status_size[0] + 30, status_y + 20), (0, 0, 0), -1)
            cv2.rectangle(frame, (status_x - 30, status_y - 50), 
                         (status_x + status_size[0] + 30, status_y + 20), status_color, 3)
            
            cv2.putText(frame, status_text, (status_x, status_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.8, status_color, 4)
            
            # ìì„¸ ì‹ ë¢°ë„
            pose_text = f"ìì„¸ ì ìˆ˜: {pose_confidence:.0%}"
            cv2.putText(frame, pose_text, (30, 115), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 2)
        
        # ğŸ“Š í•˜ë‹¨ í†µê³„
        if self.stats['frames'] > 0:
            total = self.stats['good'] + self.stats['bad']
            if total > 0:
                good_ratio = self.stats['good'] / total
                stats_text = f"Good: {self.stats['good']} | Bad: {self.stats['bad']} | ì„±ê³µë¥ : {good_ratio:.1%}"
                
                cv2.rectangle(frame, (0, height - 60), (width, height), (0, 0, 0), -1)
                cv2.putText(frame, stats_text, (30, height - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # âŒ¨ï¸ ì¡°ì‘ ê°€ì´ë“œ
        guide_text = "Q: ì¢…ë£Œ | R: ë¦¬ì…‹ | S: ìŠ¤í¬ë¦°ìƒ· | C: ìš´ë™ ë³€ê²½ | SPACE: ìˆ˜ë™ ëª¨ë“œ"
        cv2.putText(frame, guide_text, (10, height - 90), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180, 180, 180), 1)
        
        return frame
    
    def run_realtime_analysis(self, camera_id: int = 0, manual_exercise: str = None):
        """ğŸ® ì‹¤ì‹œê°„ ìë™ ë¶„ì„ ì‹¤í–‰"""
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ {camera_id} ì—´ê¸° ì‹¤íŒ¨")
            return False
        
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        print("\n" + "="*80)
        print("ğŸ¤– ìë™ ìš´ë™ ê°ì§€ + ê°ë„ ë¶„ì„ ì‹œìŠ¤í…œ")
        print("="*80)
        print("âœ¨ ê¸°ëŠ¥:")
        print("  ğŸ¤– 1ë‹¨ê³„: AIê°€ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€")
        print("  ğŸ¯ 2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ê°ë„ ë¶„ì„")
        print("  ğŸŒˆ 3ë‹¨ê³„: ì‹¤ì‹œê°„ ì´ˆë¡/ë¹¨ê°• í™”ë©´ í”¼ë“œë°±")
        print("  ğŸ“Š 4ë‹¨ê³„: í†µê³„ ë° ì„±ê³¼ ì¶”ì ")
        print("\nâŒ¨ï¸ ì¡°ì‘ë²•:")
        print("  Q: ì¢…ë£Œ | R: í†µê³„ ë¦¬ì…‹ | S: ìŠ¤í¬ë¦°ìƒ·")
        print("  C: ìˆ˜ë™ ìš´ë™ ì„ íƒ | SPACE: ìë™/ìˆ˜ë™ ëª¨ë“œ í† ê¸€")
        print("="*80)
        
        if not self.model_loaded:
            print("âš ï¸ AI ëª¨ë¸ ì—†ìŒ - ìˆ˜ë™ ëª¨ë“œë¡œ ì‹œì‘")
            if manual_exercise:
                self.current_exercise = manual_exercise
                print(f"ìˆ˜ë™ ì„ íƒ: {manual_exercise}")
        
        # ìˆ˜ë™ ìš´ë™ ì„ íƒìš©
        available_exercises = list(self.exercise_thresholds.keys())
        manual_mode = not self.model_loaded
        current_manual_idx = 0
        
        if manual_exercise and manual_exercise in available_exercises:
            current_manual_idx = available_exercises.index(manual_exercise)
            self.current_exercise = manual_exercise
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame = cv2.flip(frame, 1)  # ì…€ì¹´ ëª¨ë“œ
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.pose.process(frame_rgb)
                
                if results.pose_landmarks:
                    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    self.mp_drawing.draw_landmarks(
                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
                    
                    # ğŸ¤– 1ë‹¨ê³„: AI ìš´ë™ ê°ì§€ (ìë™ ëª¨ë“œì¼ ë•Œë§Œ)
                    if not manual_mode and self.model_loaded:
                        exercise, confidence = self.classify_exercise(frame)
                    else:
                        exercise = self.current_exercise
                        confidence = 1.0
                    
                    # ğŸ¯ 2ë‹¨ê³„: ê°ë„ ë¶„ì„
                    if exercise in self.exercise_thresholds:
                        pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, exercise)
                        
                        if pose_result['valid']:
                            # í†µê³„ ì—…ë°ì´íŠ¸
                            self.stats['frames'] += 1
                            pose_quality = pose_result['classification']
                            self.stats[pose_quality] += 1
                            
                            # ğŸŒˆ 3ë‹¨ê³„: í™”ë©´ ì˜¤ë²„ë ˆì´
                            frame = self.draw_analysis_overlay(frame, exercise, pose_result)
                        else:
                            frame = self.draw_analysis_overlay(frame, exercise, {'valid': False})
                    else:
                        frame = self.draw_analysis_overlay(frame, exercise, {'valid': False})
                else:
                    # í¬ì¦ˆ ë¯¸ê°ì§€
                    cv2.putText(frame, "ì¹´ë©”ë¼ ì•ì— ì„œì„œ ì „ì‹ ì´ ë³´ì´ë„ë¡ í•´ì£¼ì„¸ìš”", 
                               (frame.shape[1]//2 - 300, frame.shape[0]//2), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
                
                # í™”ë©´ ì¶œë ¥
                window_title = "ğŸ¤– Auto Exercise Detection + Pose Analysis"
                cv2.imshow(window_title, frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    # í†µê³„ ë¦¬ì…‹
                    self.stats = {'good': 0, 'bad': 0, 'frames': 0}
                    self.exercise_history.clear()
                    self.pose_history.clear()
                    print("ğŸ“Š í†µê³„ ë¦¬ì…‹ ì™„ë£Œ")
                elif key == ord('s'):
                    # ìŠ¤í¬ë¦°ìƒ·
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"auto_analysis_screenshot_{timestamp}.jpg"
                    cv2.imwrite(filename, frame)
                    print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filename}")
                elif key == ord('c'):
                    # ìˆ˜ë™ ìš´ë™ ë³€ê²½
                    current_manual_idx = (current_manual_idx + 1) % len(available_exercises)
                    self.current_exercise = available_exercises[current_manual_idx]
                    emoji = self.exercise_emojis.get(self.current_exercise, 'ğŸ‹ï¸')
                    print(f"ğŸ”„ ìˆ˜ë™ ì„ íƒ: {emoji} {self.current_exercise}")
                elif key == ord(' '):
                    # ìë™/ìˆ˜ë™ ëª¨ë“œ í† ê¸€
                    manual_mode = not manual_mode
                    mode = "ìˆ˜ë™" if manual_mode else "ìë™"
                    print(f"ğŸ”„ {mode} ëª¨ë“œë¡œ ë³€ê²½")
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„
            total = self.stats['good'] + self.stats['bad']
            if total > 0:
                print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
                print(f"  ğŸ¯ ì´ ë¶„ì„: {total} í”„ë ˆì„")
                print(f"  âœ… Good: {self.stats['good']} ({self.stats['good']/total:.1%})")
                print(f"  âŒ Bad: {self.stats['bad']} ({self.stats['bad']/total:.1%})")
            
            return True
    
    def analyze_single_image(self, image_path: str) -> Dict:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ìë™ ë¶„ì„"""
        if not os.path.exists(image_path):
            return {'error': f'ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}'}
        
        print(f"ğŸ¤– ìë™ ì´ë¯¸ì§€ ë¶„ì„: {image_path}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            return {'error': 'ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
        
        # í¬ì¦ˆ ê²€ì¶œ
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.pose.process(image_rgb)
        
        if not results.pose_landmarks:
            return {'error': 'í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
        
        # ğŸ¤– 1ë‹¨ê³„: AI ìš´ë™ ê°ì§€
        exercise, confidence = self.classify_exercise(image)
        print(f"ğŸ¯ AI ê°ì§€: {exercise} (ì‹ ë¢°ë„: {confidence:.1%})")
        
        # ğŸ¯ 2ë‹¨ê³„: ê°ë„ ë¶„ì„
        if exercise in self.exercise_thresholds:
            pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, exercise)
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            return {
                'success': True,
                'detected_exercise': exercise,
                'exercise_confidence': confidence,
                'pose_analysis': pose_result,
                'image_path': image_path
            }
        else:
            return {'error': f'ì§€ì›ë˜ì§€ ì•ŠëŠ” ìš´ë™: {exercise}'}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ğŸ¤– ìë™ ìš´ë™ ê°ì§€ + ê°ë„ ë¶„ì„ ì‹œìŠ¤í…œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ:
  python auto_exercise_analyzer.py --mode realtime              # ì‹¤ì‹œê°„ ìë™ ë¶„ì„
  python auto_exercise_analyzer.py --mode realtime --camera 1   # ë‹¤ë¥¸ ì¹´ë©”ë¼
  python auto_exercise_analyzer.py --mode image --input photo.jpg  # ë‹¨ì¼ ì´ë¯¸ì§€ ë¶„ì„
  python auto_exercise_analyzer.py --mode realtime --manual squat  # ìˆ˜ë™ ìš´ë™ ì„ íƒ

ğŸ¤– ì‹œìŠ¤í…œ íŠ¹ì§•:
  1ë‹¨ê³„: AIê°€ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€ (ìŠ¤ì¿¼íŠ¸, í‘¸ì‰¬ì—…, ë°ë“œë¦¬í”„íŠ¸, ë²¤ì¹˜í”„ë ˆìŠ¤, ëŸ°ì§€)
  2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ì •í™•í•œ ê°ë„ ë¶„ì„
  3ë‹¨ê³„: ì‹¤ì‹œê°„ ì´ˆë¡(Good)/ë¹¨ê°•(Bad) í™”ë©´ í”¼ë“œë°±
  4ë‹¨ê³„: í†µê³„ ë° ì„±ê³¼ ì¶”ì 

âŒ¨ï¸ ì‹¤ì‹œê°„ ì¡°ì‘:
  Q: ì¢…ë£Œ  |  R: í†µê³„ ë¦¬ì…‹  |  S: ìŠ¤í¬ë¦°ìƒ·
  C: ìˆ˜ë™ ìš´ë™ ë³€ê²½  |  SPACE: ìë™/ìˆ˜ë™ ëª¨ë“œ í† ê¸€
        """
    )
    
    parser.add_argument('--mode', type=str, default='realtime',
                       choices=['realtime', 'image'],
                       help='ë¶„ì„ ëª¨ë“œ')
    parser.add_argument('--camera', type=int, default=0,
                       help='ì¹´ë©”ë¼ ID')
    parser.add_argument('--input', type=str,
                       help='ì…ë ¥ ì´ë¯¸ì§€ íŒŒì¼ (image ëª¨ë“œìš©)')
    parser.add_argument('--manual', type=str,
                       choices=['squat', 'push_up', 'deadlift', 'bench_press', 'lunge'],
                       help='ìˆ˜ë™ ìš´ë™ ì„ íƒ (AI ê°ì§€ ê±´ë„ˆë›°ê¸°)')
    
    args = parser.parse_args()
    
    # ìë™ ë¶„ì„ê¸° ì´ˆê¸°í™”
    try:
        analyzer = AutoExerciseAnalyzer()
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return 1
    
    print("ğŸ¤– ìë™ ìš´ë™ ê°ì§€ + ê°ë„ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘!")
    print("="*60)
    print("ğŸ¯ ê¸°ëŠ¥:")
    print("  ğŸ¤– AI ìë™ ìš´ë™ ê°ì§€")
    print("  ğŸ“ Enhanced ê°ë„ ë¶„ì„")
    print("  ğŸŒˆ ì‹¤ì‹œê°„ ì´ˆë¡/ë¹¨ê°• í”¼ë“œë°±")
    print("  ğŸ“Š ì„±ê³¼ ì¶”ì ")
    
    try:
        if args.mode == 'realtime':
            print(f"\nğŸ¥ ì‹¤ì‹œê°„ ë¶„ì„ ì‹œì‘ (ì¹´ë©”ë¼ {args.camera})")
            if args.manual:
                print(f"ğŸ”§ ìˆ˜ë™ ëª¨ë“œ: {args.manual}")
            success = analyzer.run_realtime_analysis(args.camera, args.manual)
            return 0 if success else 1
            
        elif args.mode == 'image':
            if not args.input:
                print("âŒ --input ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤")
                return 1
            
            print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {args.input}")
            result = analyzer.analyze_single_image(args.input)
            
            if result.get('success', False):
                exercise = result['detected_exercise']
                exercise_conf = result['exercise_confidence']
                pose_result = result['pose_analysis']
                
                emoji = analyzer.exercise_emojis.get(exercise, 'ğŸ‹ï¸')
                print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
                print(f"ğŸ¤– AI ê°ì§€: {emoji} {exercise.upper()} (ì‹ ë¢°ë„: {exercise_conf:.1%})")
                
                if pose_result['valid']:
                    pose_quality = pose_result['classification']
                    pose_conf = pose_result['confidence']
                    
                    status_emoji = "âœ…" if pose_quality == 'good' else "âš ï¸"
                    print(f"ğŸ¯ ìì„¸ ë¶„ì„: {status_emoji} {pose_quality.upper()} (ì ìˆ˜: {pose_conf:.1%})")
                    
                    if pose_result['violations']:
                        print(f"ğŸ“ ê°œì„  í•„ìš”í•œ ë¶€ë¶„:")
                        for violation in pose_result['violations'][:3]:
                            joint = violation['joint'].replace('_', ' ').title()
                            angle = violation['angle']
                            range_min, range_max = violation['expected_range']
                            print(f"  â€¢ {joint}: {angle:.1f}Â° â†’ {range_min:.0f}-{range_max:.0f}Â°")
                else:
                    print("âŒ ìì„¸ ë¶„ì„ ì‹¤íŒ¨")
            else:
                print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return 1
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        return 0
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())