#!/usr/bin/env python3
"""
ğŸ¤– ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ê¸° - ì‚¬ì§„/ì˜ìƒ/ì‹¤ì‹œê°„ í†µí•© ë²„ì „ (ì˜ì–´ ì¶œë ¥)
1ë‹¨ê³„: AIê°€ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€
2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ìƒì„¸ ê°ë„ ë¶„ì„
3ë‹¨ê³„: ìš´ë™ë³„ ë§ì¶¤ í”¼ë“œë°± + ì´ˆë¡/ë¹¨ê°• í™”ë©´ í‘œì‹œ
4ë‹¨ê³„: ì‚¬ì§„, ì˜ìƒ, ì‹¤ì‹œê°„ ëª¨ë‘ ì§€ì›
"""
import cv2
import numpy as np
import mediapipe as mp
import time
import argparse
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import deque
import json
from datetime import datetime
import tempfile



class CompleteAutoExerciseAnalyzer:
    """ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ê¸° - ì‚¬ì§„/ì˜ìƒ/ì‹¤ì‹œê°„ í†µí•©"""
    
    def __init__(self):
        # MediaPipe ì´ˆê¸°í™”
        self.mp_pose = mp.solutions.pose
        self.pose_static = self.mp_pose.Pose(
            static_image_mode=True,
            model_complexity=2,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.pose_video = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
        self.exercise_classifier = None
        self.model_loaded = False
        self.temp_dir = tempfile.mkdtemp()
        self.load_exercise_model()
        
        # Enhanced ê°ë„ ê¸°ì¤€
        self.exercise_thresholds = {
            'squat': {
                'left_knee': {'points': [23, 25, 27], 'range': (55, 140), 'weight': 1.1, 'name_en': 'Left Knee'},
                'right_knee': {'points': [24, 26, 28], 'range': (55, 140), 'weight': 1.1, 'name_en': 'Right Knee'},
                'left_hip': {'points': [11, 23, 25], 'range': (55, 140), 'weight': 0.9, 'name_en': 'Left Hip'},
                'right_hip': {'points': [12, 24, 26], 'range': (55, 140), 'weight': 0.9, 'name_en': 'Right Hip'},
                'back_straight': {'points': [11, 23, 25], 'range': (110, 170), 'weight': 1.1, 'name_en': 'Back Straight'},
                'spine_angle': {'points': [23, 11, 13], 'range': (110, 170), 'weight': 0.9, 'name_en': 'Spine Angle'},
            },
            'push_up': {
                'left_elbow': {'points': [11, 13, 15], 'range': (40, 160), 'weight': 1.0, 'name_en': 'Left Elbow'},
                'right_elbow': {'points': [12, 14, 16], 'range': (40, 160), 'weight': 1.0, 'name_en': 'Right Elbow'},
                'body_line': {'points': [11, 23, 25], 'range': (140, 180), 'weight': 1.2, 'name_en': 'Body Line'},
                'leg_straight': {'points': [23, 25, 27], 'range': (140, 180), 'weight': 0.8, 'name_en': 'Leg Straight'},
                'shoulder_alignment': {'points': [13, 11, 23], 'range': (120, 180), 'weight': 0.6, 'name_en': 'Shoulder Align'},
                'core_stability': {'points': [11, 12, 23], 'range': (140, 180), 'weight': 1.0, 'name_en': 'Core Stability'},
            },
            'deadlift': {
                'left_knee': {'points': [23, 25, 27], 'range': (80, 140), 'weight': 0.6, 'name_en': 'Left Knee'},
                'right_knee': {'points': [24, 26, 28], 'range': (80, 140), 'weight': 0.6, 'name_en': 'Right Knee'},
                'hip_hinge': {'points': [11, 23, 25], 'range': (80, 180), 'weight': 0.7, 'name_en': 'Hip Hinge'},
                'back_straight': {'points': [11, 23, 12], 'range': (120, 180), 'weight': 1.0, 'name_en': 'Back Straight'},
                'chest_up': {'points': [23, 11, 13], 'range': (50, 140), 'weight': 0.5, 'name_en': 'Chest Up'},
                'spine_neutral': {'points': [23, 11, 24], 'range': (120, 180), 'weight': 0.8, 'name_en': 'Spine Neutral'},
            },
            'bench_press': {
                'left_elbow': {'points': [11, 13, 15], 'range': (50, 145), 'weight': 1.1, 'name_en': 'Left Elbow'},
                'right_elbow': {'points': [12, 14, 16], 'range': (50, 145), 'weight': 1.1, 'name_en': 'Right Elbow'},
                'left_shoulder': {'points': [13, 11, 23], 'range': (50, 150), 'weight': 0.9, 'name_en': 'Left Shoulder'},
                'right_shoulder': {'points': [14, 12, 24], 'range': (50, 150), 'weight': 0.9, 'name_en': 'Right Shoulder'},
                'back_arch': {'points': [11, 23, 25], 'range': (90, 170), 'weight': 0.7, 'name_en': 'Back Arch'},
                'wrist_alignment': {'points': [13, 15, 17], 'range': (70, 180), 'weight': 0.6, 'name_en': 'Wrist Align'},
            },
            'lunge': {
                'front_knee': {'points': [23, 25, 27], 'range': (70, 120), 'weight': 1.2, 'name_en': 'Front Knee'},
                'back_knee': {'points': [24, 26, 28], 'range': (120, 180), 'weight': 1.0, 'name_en': 'Back Knee'},
                'front_hip': {'points': [11, 23, 25], 'range': (70, 120), 'weight': 0.8, 'name_en': 'Front Hip'},
                'torso_upright': {'points': [11, 23, 25], 'range': (100, 180), 'weight': 1.2, 'name_en': 'Torso Upright'},
                'front_ankle': {'points': [25, 27, 31], 'range': (80, 110), 'weight': 0.8, 'name_en': 'Front Ankle'},
                'back_hip_extension': {'points': [12, 24, 26], 'range': (150, 180), 'weight': 1.0, 'name_en': 'Back Hip Ext'},
            }
        }
        
        # ìš´ë™ë³„ ë§ì¶¤ í”¼ë“œë°± ë©”ì‹œì§€ (ì˜ì–´ë¡œ ë³€ê²½)
        self.detailed_feedback = {
            'squat': {
                'left_knee': {
                    'too_low': 'Raise your left knee more (knee too bent)',
                    'too_high': 'Bend your left knee more (squat deeper)',
                    'good': 'Perfect left knee angle!'
                },
                'right_knee': {
                    'too_low': 'Raise your right knee more (knee too bent)',
                    'too_high': 'Bend your right knee more (squat deeper)',
                    'good': 'Perfect right knee angle!'
                },
                'left_hip': {
                    'too_low': 'Push your left hip back more',
                    'too_high': 'Lower your left hip more',
                    'good': 'Great left hip position!'
                },
                'right_hip': {
                    'too_low': 'Push your right hip back more',
                    'too_high': 'Lower your right hip more',
                    'good': 'Great right hip position!'
                },
                'back_straight': {
                    'too_low': 'Straighten your back (back is curved)',
                    'too_high': 'Lean forward slightly',
                    'good': 'Perfect straight back!'
                },
                'general': 'Keep knees behind toes'
            },
            'push_up': {
                'left_elbow': {
                    'too_low': 'Extend your left arm more',
                    'too_high': 'Bend your left elbow more',
                    'good': 'Perfect left arm angle!'
                },
                'right_elbow': {
                    'too_low': 'Extend your right arm more',
                    'too_high': 'Bend your right elbow more',
                    'good': 'Perfect right arm angle!'
                },
                'body_line': {
                    'too_low': 'Raise your hips (body is sagging)',
                    'too_high': 'Lower your hips (hips too high)',
                    'good': 'Perfect straight body line!'
                },
                'shoulder_alignment': {
                    'too_low': 'Keep shoulders more stable',
                    'too_high': 'Relax your shoulders naturally',
                    'good': 'Perfect shoulder alignment!'
                },
                'general': 'Keep elbows close to body'
            },
            'deadlift': {
                'left_knee': {
                    'too_low': 'Extend your left knee slightly more',
                    'too_high': 'Bend your left knee slightly',
                    'good': 'Perfect left knee!'
                },
                'right_knee': {
                    'too_low': 'Extend your right knee slightly more',
                    'too_high': 'Bend your right knee slightly',
                    'good': 'Perfect right knee!'
                },
                'hip_hinge': {
                    'too_low': 'Push your hips back more (hip hinge)',
                    'too_high': 'Lower your hips more',
                    'good': 'Perfect hip hinge movement!'
                },
                'back_straight': {
                    'too_low': 'Straighten your back - very important!',
                    'too_high': 'Relax your back naturally',
                    'good': 'Perfect straight back!'
                },
                'chest_up': {
                    'too_low': 'Lift your chest and look forward',
                    'too_high': 'Dont over-extend your chest',
                    'good': 'Perfect chest position!'
                },
                'general': 'Keep bar close to body'
            },
            'bench_press': {
                'left_elbow': {
                    'too_low': 'Extend your left arm more',
                    'too_high': 'Bend your left elbow more',
                    'good': 'Perfect left arm!'
                },
                'right_elbow': {
                    'too_low': 'Extend your right arm more',
                    'too_high': 'Bend your right elbow more',
                    'good': 'Perfect right arm!'
                },
                'left_shoulder': {
                    'too_low': 'Keep left shoulder stable',
                    'too_high': 'Relax your left shoulder',
                    'good': 'Perfect left shoulder!'
                },
                'right_shoulder': {
                    'too_low': 'Keep right shoulder stable',
                    'too_high': 'Relax your right shoulder',
                    'good': 'Perfect right shoulder!'
                },
                'back_arch': {
                    'too_low': 'Create natural back arch',
                    'too_high': 'Dont over-arch your back',
                    'good': 'Perfect back arch!'
                },
                'general': 'Control the bar slowly'
            },
            'lunge': {
                'front_knee': {
                    'too_low': 'Adjust front knee to 90 degrees (too bent)',
                    'too_high': 'Bend front knee more (to 90 degrees)',
                    'good': 'Perfect 90-degree front knee!'
                },
                'back_knee': {
                    'too_low': 'Extend your back knee more',
                    'too_high': 'Perfect back knee!',
                    'good': 'Perfect extended back knee!'
                },
                'torso_upright': {
                    'too_low': 'Keep your torso more upright',
                    'too_high': 'Perfect torso!',
                    'good': 'Perfect upright torso!'
                },
                'front_ankle': {
                    'too_low': 'Keep front ankle more stable',
                    'too_high': 'Relax your front ankle',
                    'good': 'Perfect front ankle!'
                },
                'general': 'Maintain balance and move slowly'
            }
        }
        
        # Enhanced ë¶„ë¥˜ ì„ê³„ê°’
        self.classification_thresholds = {
            'squat': 0.5,
            'push_up': 0.7,
            'deadlift': 0.8,  # ì™„í™”
            'bench_press': 0.5,
            'lunge': 0.6,
        }
        
        # ìš´ë™ ì´ëª¨ì§€ ë° ì˜ì–´ëª…
        self.exercise_info = {
            'squat': {'emoji': 'ğŸ‹ï¸â€â™€ï¸', 'name_en': 'SQUAT', 'name_display': 'Squat'},
            'push_up': {'emoji': 'ğŸ’ª', 'name_en': 'PUSH-UP', 'name_display': 'Push-up'},
            'deadlift': {'emoji': 'ğŸ‹ï¸â€â™‚ï¸', 'name_en': 'DEADLIFT', 'name_display': 'Deadlift'},
            'bench_press': {'emoji': 'ğŸ”¥', 'name_en': 'BENCH PRESS', 'name_display': 'Bench Press'},
            'lunge': {'emoji': 'ğŸš€', 'name_en': 'LUNGE', 'name_display': 'Lunge'}
        }
        
        # ìƒíƒœ ê´€ë¦¬
        self.current_exercise = "detecting..."
        self.current_pose_quality = "unknown"
        self.exercise_confidence = 0.0
        self.pose_confidence = 0.0
        
        # ì•ˆì •í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬
        self.exercise_history = deque(maxlen=10)
        self.pose_history = deque(maxlen=5)
        
        # í†µê³„
        self.stats = {'good': 0, 'bad': 0, 'frames': 0}
        
        # í™”ë©´ ìƒíƒœ (ë¶€ë“œëŸ¬ìš´ ì „í™˜)
        self.screen_color = (128, 128, 128)  # ê¸°ë³¸ íšŒìƒ‰
        self.target_color = (128, 128, 128)
        self.color_transition_speed = 0.15
        
        # íƒ€ì´ë°
        self.last_classification_time = 0
        self.classification_interval = 2.0  # 2ì´ˆë§ˆë‹¤ ìš´ë™ ë¶„ë¥˜
        
        # í”¼ë“œë°± ë©”ì‹œì§€ ê´€ë¦¬
        self.current_feedback_messages = []
        self.last_feedback_time = 0
        self.feedback_interval = 1.0  # 1ì´ˆë§ˆë‹¤ í”¼ë“œë°± ì—…ë°ì´íŠ¸
    
    def load_exercise_model(self):
        """AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ (scripts/ í´ë” ê³ ë ¤)"""
        # ê°€ëŠ¥í•œ ëª¨ë¸ ê²½ë¡œë“¤ í™•ì¸
        possible_paths = [
            "models/exercise_classifier.pkl",           # í˜„ì¬ í´ë”
            "scripts/models/exercise_classifier.pkl",   # scripts í´ë” ì•ˆ
            "../models/exercise_classifier.pkl",        # ìƒìœ„ í´ë”
            "./exercise_classifier.pkl"                 # ê°™ì€ í´ë”
        ]
        
        model_path = None
        for path in possible_paths:
            if os.path.exists(path):
                model_path = path
                break
        
        print(f"ğŸ” Searching for model in multiple locations...")
        for path in possible_paths:
            exists = "âœ…" if os.path.exists(path) else "âŒ"
            print(f"  {exists} {path}")
        
        if not model_path:
            print("âŒ No AI Model Found in any location")
            print(f"ğŸ’¡ Current directory: {os.getcwd()}")
            print(f"ğŸ’¡ Files in current dir: {os.listdir('.')}")
            if os.path.exists('scripts'):
                print(f"ğŸ’¡ Files in scripts/: {os.listdir('scripts')}")
            if os.path.exists('models'):
                print(f"ğŸ’¡ Files in models/: {os.listdir('models')}")
            if os.path.exists('scripts/models'):
                print(f"ğŸ’¡ Files in scripts/models/: {os.listdir('scripts/models')}")
            self.model_loaded = False
            return
        
        print(f"âœ… Found model at: {model_path}")
        
        try:
            print("âœ… Model file found, attempting to import...")
            try:
                from exercise_classifier import ExerciseClassificationModel
                print("âœ… Successfully imported ExerciseClassificationModel")
            except ImportError as ie:
                print(f"âŒ Import Error: {ie}")
                print("ğŸ’¡ Make sure exercise_classifier.py is in the current directory")
                # scripts í´ë”ì—ì„œ import ì‹œë„
                try:
                    import sys
                    if 'scripts' not in sys.path:
                        sys.path.append('scripts')
                    from exercise_classifier import ExerciseClassificationModel
                    print("âœ… Successfully imported from scripts folder")
                except ImportError as ie2:
                    print(f"âŒ Import from scripts also failed: {ie2}")
                    self.model_loaded = False
                    return
            
            print("âœ… Creating model instance...")
            self.exercise_classifier = ExerciseClassificationModel()
            
            print(f"âœ… Loading model from {model_path}...")
            self.model_loaded = self.exercise_classifier.load_model(model_path)
            
            if self.model_loaded:
                print("âœ… AI Exercise Classification Model Loaded Successfully")
                # ì§€ì›ë˜ëŠ” ìš´ë™ ëª©ë¡ ì¶œë ¥
                if hasattr(self.exercise_classifier, 'label_encoder'):
                    exercises = list(self.exercise_classifier.label_encoder.keys())
                    print(f"ğŸ¯ Supported exercises: {exercises}")
                
                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
                print("ğŸ§ª Testing model with dummy prediction...")
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ëŠ” ìƒëµ (ì‹¤ì œ ì´ë¯¸ì§€ê°€ í•„ìš”í•¨)
                
            else:
                print("âŒ Model Load Failed - model.load_model() returned False")
                print("ğŸ’¡ Try retraining the model: python main.py --mode train")
                
        except Exception as e:
            print(f"âŒ Model Load Error: {e}")
            import traceback
            traceback.print_exc()
            self.model_loaded = False
    
    def calculate_angle(self, p1: Tuple[float, float], p2: Tuple[float, float], p3: Tuple[float, float]) -> float:
        """ê°ë„ ê³„ì‚°"""
        try:
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]], dtype=np.float64)
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]], dtype=np.float64)
            
            v1_mag = np.linalg.norm(v1)
            v2_mag = np.linalg.norm(v2)
            
            if v1_mag < 1e-6 or v2_mag < 1e-6:
                return 180.0
            
            cos_angle = np.dot(v1, v2) / (v1_mag * v2_mag)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.degrees(np.arccos(cos_angle))
            
            return float(angle)
        except:
            return 180.0
    
    def classify_exercise(self, frame: np.ndarray) -> Tuple[str, float]:
        """ğŸ¤– 1ë‹¨ê³„: AIë¡œ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€"""
        current_time = time.time()
        
        # ë¶„ë¥˜ ì£¼ê¸° ì œì–´ (2ì´ˆë§ˆë‹¤)
        if current_time - self.last_classification_time < self.classification_interval:
            return self.current_exercise, self.exercise_confidence
        
        if not self.model_loaded:
            return "manual_mode", 0.0
        
        try:
            # ì„ì‹œ ì´ë¯¸ì§€ ì €ì¥
            temp_path = os.path.join(self.temp_dir, "temp_frame.jpg")
            cv2.imwrite(temp_path, frame)
            
            # AI ìš´ë™ ë¶„ë¥˜
            exercise, confidence = self.exercise_classifier.predict(temp_path)
            
            # íˆìŠ¤í† ë¦¬ ì•ˆì •í™”
            self.exercise_history.append((exercise, confidence))
            
            if len(self.exercise_history) >= 3:
                # ìµœê·¼ 3ê°œ ê²°ê³¼ì˜ í•©ì˜
                recent = list(self.exercise_history)[-3:]
                high_conf_predictions = [(ex, conf) for ex, conf in recent if conf > 0.6]
                
                if high_conf_predictions:
                    from collections import Counter
                    exercises = [ex for ex, conf in high_conf_predictions]
                    most_common = Counter(exercises).most_common(1)[0]
                    
                    if most_common[1] >= 2:  # 2ë²ˆ ì´ìƒ ê°ì§€
                        new_exercise = most_common[0]
                        if new_exercise != self.current_exercise:
                            self.current_exercise = new_exercise
                            self.exercise_confidence = confidence
                            exercise_info = self.exercise_info.get(new_exercise, {})
                            emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                            name_display = exercise_info.get('name_display', new_exercise)
                            print(f"AI Detected: {emoji} {name_display} (Confidence: {confidence:.1%})")
            
            self.last_classification_time = current_time
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            return self.current_exercise, self.exercise_confidence
            
        except Exception as e:
            print(f"Exercise Classification Error: {e}")
            return self.current_exercise, self.exercise_confidence
    
    def analyze_pose_angles(self, landmarks, exercise: str) -> Dict:
        """ğŸ¯ 2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ìƒì„¸ ê°ë„ ë¶„ì„"""
        if exercise not in self.exercise_thresholds:
            return {'valid': False, 'error': f'Unsupported exercise: {exercise}'}
        
        thresholds = self.exercise_thresholds[exercise]
        angles = {}
        violations = []
        weighted_violation_score = 0.0
        total_weight = 0.0
        
        for joint_name, config in thresholds.items():
            try:
                p1_idx, p2_idx, p3_idx = config['points']
                min_angle, max_angle = config['range']
                weight = config['weight']
                
                # ê°€ì‹œì„± í™•ì¸
                if (landmarks[p1_idx].visibility < 0.25 or 
                    landmarks[p2_idx].visibility < 0.25 or 
                    landmarks[p3_idx].visibility < 0.25):
                    continue
                
                p1 = (landmarks[p1_idx].x, landmarks[p1_idx].y)
                p2 = (landmarks[p2_idx].x, landmarks[p2_idx].y)
                p3 = (landmarks[p3_idx].x, landmarks[p3_idx].y)
                
                angle = self.calculate_angle(p1, p2, p3)
                angles[joint_name] = {
                    'value': angle,
                    'range': (min_angle, max_angle),
                    'weight': weight,
                    'in_range': min_angle <= angle <= max_angle,
                    'name_en': config.get('name_en', joint_name)
                }
                
                total_weight += weight
                
                if not (min_angle <= angle <= max_angle):
                    violations.append({
                        'joint': joint_name,
                        'angle': angle,
                        'expected_range': (min_angle, max_angle),
                        'weight': weight,
                        'name_en': config.get('name_en', joint_name)
                    })
                    weighted_violation_score += weight
                    
            except Exception as e:
                continue
        
        # Enhanced ë¶„ë¥˜
        violation_ratio = weighted_violation_score / max(total_weight, 1.0)
        classification_threshold = self.classification_thresholds.get(exercise, 0.6)
        is_good = violation_ratio < classification_threshold
        
        return {
            'valid': True,
            'classification': 'good' if is_good else 'bad',
            'confidence': 1.0 - violation_ratio,
            'angles': angles,
            'violations': violations,
            'violation_ratio': violation_ratio,
            'threshold': classification_threshold
        }
    
    def generate_detailed_feedback(self, exercise: str, pose_result: Dict) -> List[str]:
        """ğŸ—£ï¸ ìš´ë™ë³„ ìƒì„¸ í”¼ë“œë°± ìƒì„± (ì˜ì–´)"""
        current_time = time.time()
        
        # í”¼ë“œë°± ì£¼ê¸° ì œí•œ
        if current_time - self.last_feedback_time < self.feedback_interval:
            return self.current_feedback_messages
        
        messages = []
        
        if not pose_result.get('valid', False):
            messages.append("Cannot recognize pose")
            return messages
        
        violations = pose_result.get('violations', [])
        exercise_feedback = self.detailed_feedback.get(exercise, {})
        
        if not violations:
            # ëª¨ë“  ìì„¸ê°€ ì™„ë²½í•œ ê²½ìš°
            exercise_info = self.exercise_info.get(exercise, {})
            name_display = exercise_info.get('name_display', exercise)
            messages.append(f"Perfect {name_display} form! ğŸ‘")
            messages.append("Keep this form!")
        else:
            # ìœ„ë°˜ì‚¬í•­ì´ ìˆëŠ” ê²½ìš° - ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
            violations_sorted = sorted(violations, key=lambda x: x['weight'], reverse=True)
            
            for i, violation in enumerate(violations_sorted[:3]):  # ìƒìœ„ 3ê°œë§Œ
                joint = violation['joint']
                angle = violation['angle']
                min_angle, max_angle = violation['expected_range']
                name_en = violation.get('name_en', joint)
                
                joint_feedback = exercise_feedback.get(joint, {})
                
                if angle < min_angle:
                    # ê°ë„ê°€ ë„ˆë¬´ ì‘ìŒ
                    message = joint_feedback.get('too_low', f'Increase {name_en} angle')
                elif angle > max_angle:
                    # ê°ë„ê°€ ë„ˆë¬´ í¼
                    message = joint_feedback.get('too_high', f'Decrease {name_en} angle')
                else:
                    message = joint_feedback.get('good', f'{name_en} is good!')
                
                messages.append(f"âš ï¸ {message}")
                
                # êµ¬ì²´ì ì¸ ê°ë„ ì •ë³´ ì¶”ê°€
                if i == 0:  # ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì œë§Œ ê°ë„ í‘œì‹œ
                    messages.append(f"   Current: {angle:.0f}Â° â†’ Target: {min_angle:.0f}-{max_angle:.0f}Â°")
            
            # ì¼ë°˜ì ì¸ ìš´ë™ë³„ ì¡°ì–¸ ì¶”ê°€
            general_advice = exercise_feedback.get('general', '')
            if general_advice and len(violations_sorted) <= 2:
                messages.append(f"ğŸ’¡ {general_advice}")
        
        self.current_feedback_messages = messages
        self.last_feedback_time = current_time
        return messages
    
    def update_screen_color(self, pose_quality: str):
        """ğŸŒˆ ì´ˆë¡/ë¹¨ê°• í™”ë©´ ìƒ‰ìƒ ì—…ë°ì´íŠ¸"""
        if pose_quality == 'good':
            self.target_color = (0, 255, 0)      # ì´ˆë¡ìƒ‰
        elif pose_quality == 'bad':
            self.target_color = (0, 0, 255)      # ë¹¨ê°„ìƒ‰
        elif pose_quality == 'detecting':
            self.target_color = (255, 255, 0)    # ë…¸ë€ìƒ‰
        else:
            self.target_color = (128, 128, 128)  # íšŒìƒ‰
        
        # ë¶€ë“œëŸ¬ìš´ ìƒ‰ìƒ ì „í™˜
        for i in range(3):
            current = self.screen_color[i]
            target = self.target_color[i]
            diff = target - current
            self.screen_color = tuple(
                int(current + diff * self.color_transition_speed) if j == i 
                else self.screen_color[j] for j in range(3)
            )
    
    def draw_enhanced_overlay(self, frame: np.ndarray, exercise: str, pose_result: Dict) -> np.ndarray:
        """âœ¨ í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ í™”ë©´ ì˜¤ë²„ë ˆì´ (ì˜ì–´ í…ìŠ¤íŠ¸)"""
        height, width = frame.shape[:2]
        
        # ğŸŒˆ ì „ì²´ í™”ë©´ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ ë° í…Œë‘ë¦¬
        if pose_result.get('valid', False):
            pose_quality = pose_result['classification']
            self.update_screen_color(pose_quality)
            
            # íˆ¬ëª…í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´
            overlay = frame.copy()
            cv2.rectangle(overlay, (0, 0), (width, height), self.screen_color, -1)
            cv2.addWeighted(overlay, 0.1, frame, 0.9, 0, frame)
        
        # ğŸ¯ ë‘êº¼ìš´ í…Œë‘ë¦¬
        border_thickness = 30
        cv2.rectangle(frame, (0, 0), (width, height), self.screen_color, border_thickness)
        
        # ğŸ“ ì™¼ìª½ ìœ„: ìš´ë™ ì¢…ë¥˜ í‘œì‹œ
        exercise_info = self.exercise_info.get(exercise, {})
        if exercise != "detecting..." and exercise != "manual_mode":
            emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
            name_display = exercise_info.get('name_display', exercise)
            name_en = exercise_info.get('name_en', exercise.upper())
            
            # ë°°ê²½ ë°•ìŠ¤
            cv2.rectangle(frame, (40, 40), (400, 140), (0, 0, 0), -1)
            cv2.rectangle(frame, (40, 40), (400, 140), self.screen_color, 3)
            
            # ìš´ë™ëª… í‘œì‹œ (ê¸€ì í¬ê¸° ì¤„ì„)
            exercise_text = f"{emoji} {name_display}"
            cv2.putText(frame, exercise_text, (60, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)  # 1.2 -> 0.8
            
            # ì˜ì–´ëª… í‘œì‹œ (ê¸€ì í¬ê¸° ì¤„ì„)
            cv2.putText(frame, name_en, (60, 105), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)  # 0.8 -> 0.6
            
            # ì‹ ë¢°ë„ í‘œì‹œ (ê¸€ì í¬ê¸° ì¤„ì„)
            if self.model_loaded:
                confidence_text = f"AI: {self.exercise_confidence:.0%}"
            else:
                confidence_text = "Manual Mode"
            cv2.putText(frame, confidence_text, (60, 125), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 2)  # 0.6 -> 0.5
            
        elif exercise == "detecting...":
            cv2.rectangle(frame, (40, 40), (300, 100), (0, 0, 0), -1)
            cv2.rectangle(frame, (40, 40), (300, 100), (255, 255, 0), 3)
            cv2.putText(frame, "ğŸ¤– Detecting...", (60, 75), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)  # ê¸€ì í¬ê¸° ì¤„ì„
        else:
            cv2.rectangle(frame, (40, 40), (320, 100), (0, 0, 0), -1)
            cv2.rectangle(frame, (40, 40), (320, 100), (128, 128, 128), 3)
            cv2.putText(frame, "âš™ï¸ No AI Model", (60, 75), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)  # ê¸€ì í¬ê¸° ì¤„ì„
        
        # ğŸ¯ ì¤‘ì•™ ìƒíƒœ ë©”ì‹œì§€
        if pose_result.get('valid', False):
            pose_quality = pose_result['classification']
            confidence = pose_result['confidence']
            
            if pose_quality == 'good':
                status_text = "Perfect Form! ğŸ‘"
                status_color = (0, 255, 0)
            else:
                status_text = "Form Needs Work âš ï¸"
                status_color = (0, 0, 255)
            
            # ì¤‘ì•™ ìƒíƒœ í‘œì‹œ
            status_size = cv2.getTextSize(status_text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)[0]
            status_x = (width - status_size[0]) // 2
            status_y = height // 2 - 80
            
            cv2.rectangle(frame, (status_x - 30, status_y - 40), 
                         (status_x + status_size[0] + 30, status_y + 20), (0, 0, 0), -1)
            cv2.rectangle(frame, (status_x - 30, status_y - 40), 
                         (status_x + status_size[0] + 30, status_y + 20), status_color, 4)
            
            cv2.putText(frame, status_text, (status_x, status_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, status_color, 2)  # 1.5 -> 1.0
            
            # ì‹ ë¢°ë„ ì ìˆ˜ (ê¸€ì í¬ê¸° ì¤„ì„)
            score_text = f"Form Score: {confidence:.0%}"
            score_size = cv2.getTextSize(score_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]  # 0.8 -> 0.6
            score_x = (width - score_size[0]) // 2
            cv2.putText(frame, score_text, (score_x, status_y + 40), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)  # 0.8 -> 0.6
        
        # ğŸ“ ì™¼ìª½ ì•„ë˜: ìƒì„¸ í”¼ë“œë°± ë©”ì‹œì§€
        if exercise in self.exercise_thresholds:
            feedback_messages = self.generate_detailed_feedback(exercise, pose_result)
            
            if feedback_messages:
                # í”¼ë“œë°± ì˜ì—­ ë°°ê²½
                feedback_height = len(feedback_messages) * 35 + 60
                cv2.rectangle(frame, (40, height - feedback_height - 40), 
                             (width - 40, height - 40), (0, 0, 0), -1)
                cv2.rectangle(frame, (40, height - feedback_height - 40), 
                             (width - 40, height - 40), self.screen_color, 3)
                
                # í”¼ë“œë°± ì œëª© (ê¸€ì í¬ê¸° ì¤„ì„)
                cv2.putText(frame, "ğŸ’¬ Feedback:", (60, height - feedback_height - 10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)  # 0.7 -> 0.6
                
                # í”¼ë“œë°± ë©”ì‹œì§€ë“¤ (ê¸€ì í¬ê¸° ì¤„ì„)
                for i, message in enumerate(feedback_messages[:5]):  # ìµœëŒ€ 5ê°œ
                    y_pos = height - feedback_height + 20 + (i * 30)  # 35 -> 30 (ì¤„ê°„ê²© ì¤„ì„)
                    
                    # ë©”ì‹œì§€ ìƒ‰ìƒ ê²°ì •
                    if "Perfect" in message or "ğŸ‘" in message:
                        msg_color = (0, 255, 0)  # ì´ˆë¡ìƒ‰
                    elif "âš ï¸" in message:
                        msg_color = (0, 100, 255)  # ì£¼í™©ìƒ‰
                    elif "ğŸ’¡" in message:
                        msg_color = (255, 255, 0)  # ë…¸ë€ìƒ‰
                    else:
                        msg_color = (255, 255, 255)  # í°ìƒ‰
                    
                    cv2.putText(frame, message, (60, y_pos), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, msg_color, 2)  # 0.6 -> 0.5
        
        # ğŸ“Š ì˜¤ë¥¸ìª½ ìœ„: í†µê³„ ì •ë³´
        if self.stats['frames'] > 0:
            total = self.stats['good'] + self.stats['bad']
            if total > 0:
                good_ratio = self.stats['good'] / total
                
                # í†µê³„ ë°°ê²½
                cv2.rectangle(frame, (width - 300, 40), (width - 40, 140), (0, 0, 0), -1)
                cv2.rectangle(frame, (width - 300, 40), (width - 40, 140), (255, 255, 255), 2)
                
                # í†µê³„ í…ìŠ¤íŠ¸ (ê¸€ì í¬ê¸° ì¤„ì„)
                cv2.putText(frame, "ğŸ“Š Stats", (width - 280, 70), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)  # 0.6 -> 0.5
                
                stats_text = f"Good: {self.stats['good']} | Bad: {self.stats['bad']}"
                cv2.putText(frame, stats_text, (width - 280, 90), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)  # 0.5 -> 0.4
                
                ratio_text = f"Success: {good_ratio:.1%}"
                cv2.putText(frame, ratio_text, (width - 280, 110), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0) if good_ratio > 0.7 else (255, 255, 255), 1)  # 0.5 -> 0.4
        
        # âŒ¨ï¸ í•˜ë‹¨ ì¡°ì‘ ê°€ì´ë“œ (ê¸€ì í¬ê¸° ì¤„ì„)
        guide_text = "Q: Quit  |  R: Reset  |  S: Screenshot  |  C: Change Exercise  |  SPACE: Toggle Mode"
        cv2.putText(frame, guide_text, (50, height - 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, (180, 180, 180), 1)  # 0.5 -> 0.4
        
        return frame
    
    def analyze_single_image(self, image_path: str) -> Dict:
        """ğŸ“· ë‹¨ì¼ ì´ë¯¸ì§€ ì™„ì „ ìë™ ë¶„ì„"""
        if not os.path.exists(image_path):
            return {'error': f'Image file not found: {image_path}'}
        
        print(f"ğŸ“· Starting automatic image analysis: {os.path.basename(image_path)}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            return {'error': 'Cannot read image'}
        
        # í¬ì¦ˆ ê²€ì¶œ (ì •ì  ì´ë¯¸ì§€ìš© ê³ ì •ë°€ ëª¨ë¸)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.pose_static.process(image_rgb)
        
        if not results.pose_landmarks:
            return {'error': 'Cannot detect pose'}
        
        # ğŸ¤– 1ë‹¨ê³„: AI ìš´ë™ ê°ì§€
        exercise, confidence = self.classify_exercise(image)
        exercise_info = self.exercise_info.get(exercise, {})
        emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
        name_display = exercise_info.get('name_display', exercise)
        
        print(f"ğŸ¯ AI Detection: {emoji} {name_display} (Confidence: {confidence:.1%})")
        
        # ğŸ¯ 2ë‹¨ê³„: ê°ë„ ë¶„ì„
        if exercise in self.exercise_thresholds:
            pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, exercise)
            
            # ğŸ—£ï¸ 3ë‹¨ê³„: ìƒì„¸ í”¼ë“œë°± ìƒì„±
            feedback_messages = self.generate_detailed_feedback(exercise, pose_result)
            
            # ğŸ“¸ 4ë‹¨ê³„: ì£¼ì„ ì´ë¯¸ì§€ ìƒì„±
            annotated_image = image.copy()
            
            # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
            self.mp_drawing.draw_landmarks(
                annotated_image, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
            
            # ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°
            annotated_image = self.draw_enhanced_overlay(annotated_image, exercise, pose_result)
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            return {
                'success': True,
                'image_path': image_path,
                'detected_exercise': exercise,
                'exercise_info': exercise_info,
                'exercise_confidence': confidence,
                'pose_analysis': pose_result,
                'feedback_messages': feedback_messages,
                'original_image': image,
                'annotated_image': annotated_image,
                'analysis_timestamp': datetime.now().isoformat()
            }
        else:
            return {'error': f'Unsupported exercise: {exercise}'}
    
    def analyze_video_file(self, video_path: str, output_path: str = None) -> Dict:
        """ğŸ¬ ì˜ìƒ íŒŒì¼ ì™„ì „ ìë™ ë¶„ì„"""
        if not os.path.exists(video_path):
            return {'error': f'Video file not found: {video_path}'}
        
        print(f"ğŸ¬ Starting automatic video analysis: {os.path.basename(video_path)}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return {'error': 'Cannot open video file'}
        
        # ì˜ìƒ ì •ë³´
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“¹ Video Info: {width}x{height}, {fps}fps, {total_frames} frames")
        
        # ì¶œë ¥ ì˜ìƒ ì„¤ì •
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        frame_results = []
        exercise_detections = {}
        stats = {'good': 0, 'bad': 0, 'total': 0}
        
        # ì„ì‹œë¡œ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        self.exercise_history.clear()
        current_exercise = "detecting..."
        
        try:
            frame_count = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # RGB ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # í¬ì¦ˆ ê²€ì¶œ
                results = self.pose_video.process(frame_rgb)
                
                if results.pose_landmarks:
                    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    self.mp_drawing.draw_landmarks(
                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
                    
                    # ğŸ¤– ìš´ë™ ê°ì§€ (ì˜ìƒìš©)
                    exercise, confidence = self.classify_exercise(frame)
                    
                    # ìš´ë™ ê°ì§€ í†µê³„
                    if exercise != "detecting..." and exercise != "manual_mode":
                        if exercise not in exercise_detections:
                            exercise_detections[exercise] = 0
                        exercise_detections[exercise] += 1
                        current_exercise = exercise
                    
                    # ğŸ¯ ê°ë„ ë¶„ì„
                    if current_exercise in self.exercise_thresholds:
                        pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, current_exercise)
                        
                        if pose_result['valid']:
                            pose_quality = pose_result['classification']
                            stats[pose_quality] += 1
                            stats['total'] += 1
                            
                            # ğŸ—£ï¸ í”¼ë“œë°± ìƒì„±
                            feedback_messages = self.generate_detailed_feedback(current_exercise, pose_result)
                            
                            # âœ¨ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°
                            frame = self.draw_enhanced_overlay(frame, current_exercise, pose_result)
                            
                            # ê²°ê³¼ ì €ì¥
                            frame_results.append({
                                'frame': frame_count,
                                'timestamp': frame_count / fps,
                                'exercise': current_exercise,
                                'classification': pose_quality,
                                'confidence': pose_result['confidence'],
                                'feedback': feedback_messages[:3]  # ìƒìœ„ 3ê°œë§Œ
                            })
                        else:
                            frame = self.draw_enhanced_overlay(frame, current_exercise, {'valid': False})
                    else:
                        frame = self.draw_enhanced_overlay(frame, current_exercise, {'valid': False})
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_count % (fps * 5) == 0:  # 5ì´ˆë§ˆë‹¤
                    progress = (frame_count / total_frames) * 100
                    print(f"ğŸ“Š Analysis Progress: {progress:.1f}%")
                
                # ì¶œë ¥ ì˜ìƒì— ì“°ê¸°
                if output_path:
                    out.write(frame)
                
                frame_count += 1
                
        except Exception as e:
            print(f"âŒ Video analysis error: {e}")
            return {'error': f'Video analysis failed: {str(e)}'}
        finally:
            cap.release()
            if output_path:
                out.release()
        
        # ê°€ì¥ ë§ì´ ê°ì§€ëœ ìš´ë™ ì°¾ê¸°
        main_exercise = max(exercise_detections.items(), key=lambda x: x[1])[0] if exercise_detections else "unknown"
        
        # ê²°ê³¼ ìš”ì•½
        success_rate = (stats['good'] / max(stats['total'], 1)) * 100
        
        print(f"\nğŸ‰ Video analysis complete!")
        print(f"ğŸ¯ Main exercise: {self.exercise_info.get(main_exercise, {}).get('name_display', main_exercise)}")
        print(f"ğŸ“Š Analysis results: Good {stats['good']} frames, Bad {stats['bad']} frames")
        print(f"ğŸ¯ Success rate: {success_rate:.1f}%")
        
        return {
            'success': True,
            'video_path': video_path,
            'output_path': output_path,
            'main_exercise': main_exercise,
            'exercise_detections': exercise_detections,
            'stats': stats,
            'success_rate': success_rate,
            'frame_results': frame_results,
            'total_frames_analyzed': len(frame_results),
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def run_realtime_analysis(self, camera_id: int = 0, manual_exercise: str = None):
        """ğŸ® ì‹¤ì‹œê°„ ì™„ì „ ìë™ ë¶„ì„"""
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            print(f"âŒ Failed to open camera {camera_id}")
            return False
        
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)

        cv2.namedWindow('Exercise Analysis', cv2.WINDOW_NORMAL)
        cv2.resizeWindow('Exercise Analysis', 1600, 1200)  # ë” í° ì°½ í¬ê¸°
        
        print("\n" + "="*80)
        print("ğŸ¤– Complete Automated Exercise Analysis System")
        print("="*80)
        print("âœ¨ Features:")
        print("  ğŸ¤– Step 1: AI automatically detects exercise type")
        print("  ğŸ¯ Step 2: Precise angle analysis based on detected exercise")
        print("  ğŸ—£ï¸ Step 3: Exercise-specific detailed feedback")
        print("  ğŸŒˆ Step 4: Real-time green/red screen + border")
        print("  ğŸ“Š Step 5: Real-time statistics and performance tracking")
        print("\nğŸ“ Screen Layout:")
        print("  â€¢ Top Left: Detected exercise type")
        print("  â€¢ Bottom Left: Detailed feedback messages")
        print("  â€¢ Top Right: Exercise statistics")
        print("  â€¢ Center: Form status (Good/Bad)")
        print("  â€¢ Overall: Green/red border + background")
        print("\nâŒ¨ï¸ Controls:")
        print("  Q: Quit | R: Reset Stats | S: Screenshot")
        print("  C: Manual Exercise Selection | SPACE: Auto/Manual Mode Toggle")
        print("="*80)
        
        # ëª¨ë¸ ìƒíƒœ í™•ì¸ ë° ê¸°ë³¸ ìš´ë™ ì„¤ì •
        if not self.model_loaded:
            print("âš ï¸ No AI Model Found - Starting with default exercise")
            # AI ëª¨ë¸ì´ ì—†ì–´ë„ ê¸°ë³¸ ìš´ë™ìœ¼ë¡œ ì‹œì‘ (ìˆ˜ë™ ëª¨ë“œê°€ ì•„ë‹˜)
            if manual_exercise:
                self.current_exercise = manual_exercise
            else:
                self.current_exercise = 'squat'  # ê¸°ë³¸ê°’ìœ¼ë¡œ ìŠ¤ì¿¼íŠ¸ ì„¤ì •
            
            exercise_info = self.exercise_info.get(self.current_exercise, {})
            print(f"Default Exercise: {exercise_info.get('emoji', 'ğŸ‹ï¸')} {exercise_info.get('name_display', self.current_exercise)}")
            print("ğŸ’¡ You can change exercise with 'C' key or train AI model for auto-detection")
        
        # ìˆ˜ë™ ìš´ë™ ì„ íƒìš©
        available_exercises = list(self.exercise_thresholds.keys())
        manual_mode = False  # ê¸°ë³¸ì ìœ¼ë¡œ ìë™ ëª¨ë“œ (AI ì—†ì–´ë„ í˜„ì¬ ì„¤ì •ëœ ìš´ë™ìœ¼ë¡œ ë¶„ì„)
        current_manual_idx = 0
        
        if self.current_exercise in available_exercises:
            current_manual_idx = available_exercises.index(self.current_exercise)
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame = cv2.flip(frame, 1)  # ì…€ì¹´ ëª¨ë“œ
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.pose_video.process(frame_rgb)
                
                if results.pose_landmarks:
                    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    self.mp_drawing.draw_landmarks(
                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
                    
                    # ğŸ¤– 1ë‹¨ê³„: AI ìš´ë™ ê°ì§€ (AI ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ)
                    if self.model_loaded and not manual_mode:
                        exercise, confidence = self.classify_exercise(frame)
                    else:
                        # AI ëª¨ë¸ì´ ì—†ê±°ë‚˜ ìˆ˜ë™ ëª¨ë“œì¼ ë•ŒëŠ” í˜„ì¬ ì„¤ì •ëœ ìš´ë™ ì‚¬ìš©
                        exercise = self.current_exercise
                        confidence = 1.0
                    
                    # ğŸ¯ 2ë‹¨ê³„: ê°ë„ ë¶„ì„
                    if exercise in self.exercise_thresholds:
                        pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, exercise)
                        
                        if pose_result['valid']:
                            # í†µê³„ ì—…ë°ì´íŠ¸
                            self.stats['frames'] += 1
                            pose_quality = pose_result['classification']
                            self.stats[pose_quality] += 1
                            
                            # âœ¨ 3-4ë‹¨ê³„: í”¼ë“œë°± + í™”ë©´ ì˜¤ë²„ë ˆì´
                            frame = self.draw_enhanced_overlay(frame, exercise, pose_result)
                        else:
                            frame = self.draw_enhanced_overlay(frame, exercise, {'valid': False})
                    else:
                        frame = self.draw_enhanced_overlay(frame, exercise, {'valid': False})
                else:
                    # í¬ì¦ˆ ë¯¸ê°ì§€
                    cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (255, 255, 0), 30)
                    message = "Stand in front of camera (full body visible)"
                    text_size = cv2.getTextSize(message, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]
                    text_x = (frame.shape[1] - text_size[0]) // 2
                    text_y = frame.shape[0] // 2
                    
                    cv2.rectangle(frame, (text_x - 20, text_y - 40), 
                                 (text_x + text_size[0] + 20, text_y + 10), (0, 0, 0), -1)
                    cv2.putText(frame, message, (text_x, text_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
                
                # í”„ë ˆì„ í¬ê¸° ì¡°ì • (ë” í¬ê²Œ í‘œì‹œ)
                display_frame = frame.copy()
                height, width = display_frame.shape[:2]
                
                # ì›í•˜ëŠ” í‘œì‹œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                target_width = 1280
                target_height = 960
                
                # ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ë¦¬ì‚¬ì´ì¦ˆ
                scale = min(target_width / width, target_height / height)
                new_width = int(width * scale)
                new_height = int(height * scale)
                
                display_frame = cv2.resize(display_frame, (new_width, new_height))
                
                # í™”ë©´ ì¶œë ¥
                window_title = "ğŸ¤– Complete Automated Exercise Analysis System"
                cv2.imshow(window_title, display_frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    # í†µê³„ ë¦¬ì…‹
                    self.stats = {'good': 0, 'bad': 0, 'frames': 0}
                    self.exercise_history.clear()
                    self.pose_history.clear()
                    print("ğŸ“Š Statistics Reset")
                elif key == ord('s'):
                    # ìŠ¤í¬ë¦°ìƒ·
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"complete_analysis_screenshot_{timestamp}.jpg"
                    cv2.imwrite(filename, frame)
                    print(f"ğŸ“¸ Screenshot saved: {filename}")
                elif key == ord('c'):
                    # ìˆ˜ë™ ìš´ë™ ë³€ê²½
                    current_manual_idx = (current_manual_idx + 1) % len(available_exercises)
                    self.current_exercise = available_exercises[current_manual_idx]
                    exercise_info = self.exercise_info.get(self.current_exercise, {})
                    emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                    name_display = exercise_info.get('name_display', self.current_exercise)
                    print(f"ğŸ”„ Manual Selection: {emoji} {name_display}")
                elif key == ord(' '):
                    # ìë™/ìˆ˜ë™ ëª¨ë“œ í† ê¸€ (AI ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ)
                    if self.model_loaded:
                        manual_mode = not manual_mode
                        mode = "Manual" if manual_mode else "AI Auto"
                        print(f"ğŸ”„ Changed to {mode} Mode")
                    else:
                        print("ğŸ’¡ AI model not available - Use 'C' to change exercise manually")
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ User Interrupted")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„
            total = self.stats['good'] + self.stats['bad']
            if total > 0:
                success_rate = (self.stats['good'] / total) * 100
                print(f"\nğŸ“Š Final Statistics:")
                print(f"  ğŸ¯ Total Analysis: {total} frames")
                print(f"  âœ… Good: {self.stats['good']} ({success_rate:.1f}%)")
                print(f"  âŒ Bad: {self.stats['bad']} ({100-success_rate:.1f}%)")
                print(f"  ğŸ¯ Exercise-specific analysis complete!")
            
            return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ğŸ¤– Complete Automated Exercise Analyzer - Photo/Video/Realtime',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ Complete Automation Features:
  Step 1: ğŸ¤– AI automatically detects exercise type (Squat, Push-up, Deadlift, Bench Press, Lunge)
  Step 2: ğŸ¯ Precise angle analysis based on detected exercise
  Step 3: ğŸ—£ï¸ Exercise-specific detailed feedback
  Step 4: ğŸŒˆ Real-time green/red screen + border
  Step 5: ğŸ“Š Real-time statistics and performance tracking

ğŸ“ Screen Layout:
  â€¢ Top Left: Detected exercise type + confidence
  â€¢ Bottom Left: Detailed feedback messages (angle-specific advice)
  â€¢ Top Right: Exercise statistics (Good/Bad ratio)
  â€¢ Center: Form status (Perfect Form! / Form Needs Work)
  â€¢ Overall: Green(Good)/Red(Bad) border + background

ğŸ¯ Usage Examples:
  # Real-time complete auto analysis
  python auto_exercise_analyzer.py --mode realtime
  
  # Real-time + manual exercise selection
  python auto_exercise_analyzer.py --mode realtime --manual squat
  
  # Photo complete auto analysis
  python auto_exercise_analyzer.py --mode image --input photo.jpg
  
  # Video complete auto analysis
  python auto_exercise_analyzer.py --mode video --input video.mp4 --output analyzed.mp4

âŒ¨ï¸ Real-time Controls:
  Q: Quit  |  R: Reset Stats  |  S: Screenshot
  C: Change Exercise (Manual)  |  SPACE: Auto/Manual Mode Toggle

ğŸ‹ï¸ Supported Exercises & Detailed Feedback:
  ğŸ‹ï¸â€â™€ï¸ Squat: Knee/hip angles, keep back straight, knees behind toes
  ğŸ’ª Push-up: Elbow angles, straight body line, shoulder stability
  ğŸ‹ï¸â€â™‚ï¸ Deadlift: Hip hinge, straight back, knee angles (relaxed criteria)
  ğŸ”¥ Bench Press: Elbow/shoulder angles, back arch
  ğŸš€ Lunge: Front knee 90Â°, extend back knee, upright torso

ğŸ’¡ AI Model Required:
  With models/exercise_classifier.pkl: Complete automation
  Without model: Manual exercise selection mode
        """
    )
    
    parser.add_argument('--mode', type=str, default='realtime',
                       choices=['realtime', 'image', 'video'],
                       help='Analysis mode: realtime, image, or video')
    parser.add_argument('--input', type=str,
                       help='Input file path (for image/video mode)')
    parser.add_argument('--output', type=str,
                       help='Output file path (for video mode)')
    parser.add_argument('--camera', type=int, default=0,
                       help='Camera ID (for realtime mode)')
    parser.add_argument('--manual', type=str,
                       choices=['squat', 'push_up', 'deadlift', 'bench_press', 'lunge'],
                       help='Manual exercise selection (skip AI detection)')
    
    args = parser.parse_args()
    
    # ì™„ì „ ìë™í™” ë¶„ì„ê¸° ì´ˆê¸°í™”
    try:
        analyzer = CompleteAutoExerciseAnalyzer()
    except Exception as e:
        print(f"âŒ System initialization failed: {e}")
        return 1
    
    print("ğŸ¤– Complete Automated Exercise Analysis System Starting!")
    print("="*80)
    print("ğŸ¯ Key Features:")
    print("  ğŸ¤– AI automatic exercise detection (5 exercises)")
    print("  ğŸ“ Precise angle analysis")
    print("  ğŸ—£ï¸ Exercise-specific detailed feedback")
    print("  ğŸŒˆ Real-time green/red feedback")
    print("  ğŸ“Š Performance tracking")
    print("  ğŸ“· Photo/ğŸ¬ Video/ğŸ® Real-time support")
    
    try:
        if args.mode == 'realtime':
            print(f"\nğŸ® Starting real-time analysis (Camera {args.camera})")
            if args.manual:
                exercise_info = analyzer.exercise_info.get(args.manual, {})
                emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                name_display = exercise_info.get('name_display', args.manual)
                print(f"ğŸ”§ Manual Mode: {emoji} {name_display}")
            success = analyzer.run_realtime_analysis(args.camera, args.manual)
            return 0 if success else 1
            
        elif args.mode == 'image':
            if not args.input:
                print("âŒ --input option required (image file path)")
                return 1
            
            print(f"\nğŸ“· Starting image analysis: {args.input}")
            result = analyzer.analyze_single_image(args.input)
            
            if result.get('success', False):
                # ê²°ê³¼ ì¶œë ¥
                exercise_info = result['exercise_info']
                emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                name_display = exercise_info.get('name_display', 'unknown')
                exercise_conf = result['exercise_confidence']
                pose_result = result['pose_analysis']
                
                print(f"\nğŸ‰ Image analysis complete!")
                print(f"ğŸ¤– AI Detection: {emoji} {name_display} (Confidence: {exercise_conf:.1%})")
                
                if pose_result['valid']:
                    pose_quality = pose_result['classification']
                    pose_conf = pose_result['confidence']
                    
                    status_emoji = "âœ…" if pose_quality == 'good' else "âš ï¸"
                    print(f"ğŸ¯ Form Analysis: {status_emoji} {pose_quality.upper()} (Score: {pose_conf:.1%})")
                    
                    # í”¼ë“œë°± ë©”ì‹œì§€ ì¶œë ¥
                    feedback_messages = result['feedback_messages']
                    if feedback_messages:
                        print(f"\nğŸ’¬ Detailed Feedback:")
                        for i, message in enumerate(feedback_messages[:5], 1):
                            print(f"  {i}. {message}")
                    
                    # ìœ„ë°˜ì‚¬í•­ ì¶œë ¥
                    violations = pose_result.get('violations', [])
                    if violations:
                        print(f"\nğŸ“ Angle Analysis:")
                        for violation in violations[:3]:
                            joint_en = violation.get('name_en', violation['joint'])
                            angle = violation['angle']
                            range_min, range_max = violation['expected_range']
                            print(f"  â€¢ {joint_en}: {angle:.1f}Â° â†’ Target: {range_min:.0f}-{range_max:.0f}Â°")
                
                # ì£¼ì„ ì´ë¯¸ì§€ í‘œì‹œ
                annotated_image = result['annotated_image']
                
                # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (í™”ë©´ì— ë§ê²Œ)
                height, width = annotated_image.shape[:2]
                if width > 1200:
                    scale = 1200 / width
                    new_width = int(width * scale)
                    new_height = int(height * scale)
                    annotated_image = cv2.resize(annotated_image, (new_width, new_height))
                
                window_title = f"Complete Auto Analysis Result: {emoji} {name_display}"
                cv2.imshow(window_title, annotated_image)
                
                print(f"\nğŸ–¼ï¸ Analysis result image displayed... (Press any key to close)")
                cv2.waitKey(0)
                cv2.destroyAllWindows()
                
            else:
                print(f"âŒ Image analysis failed: {result.get('error', 'Unknown error')}")
                return 1
                
        elif args.mode == 'video':
            if not args.input:
                print("âŒ --input option required (video file path)")
                return 1
            
            print(f"\nğŸ¬ Starting video analysis: {args.input}")
            if args.output:
                print(f"ğŸ“ Output path: {args.output}")
            
            result = analyzer.analyze_video_file(args.input, args.output)
            
            if result.get('success', False):
                # ê²°ê³¼ ì¶œë ¥
                main_exercise = result['main_exercise']
                exercise_info = analyzer.exercise_info.get(main_exercise, {})
                emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                name_display = exercise_info.get('name_display', main_exercise)
                
                stats = result['stats']
                success_rate = result['success_rate']
                total_analyzed = result['total_frames_analyzed']
                
                print(f"\nğŸ‰ Video analysis complete!")
                print(f"ğŸ¯ Main exercise: {emoji} {name_display}")
                print(f"ğŸ“Š Analysis results:")
                print(f"  â€¢ Total analyzed frames: {total_analyzed}")
                print(f"  â€¢ âœ… Good form: {stats['good']} frames")
                print(f"  â€¢ âŒ Bad form: {stats['bad']} frames")
                print(f"  â€¢ ğŸ¯ Success rate: {success_rate:.1f}%")
                
                # ìš´ë™ ê°ì§€ í†µê³„
                exercise_detections = result['exercise_detections']
                if len(exercise_detections) > 1:
                    print(f"\nğŸ“ˆ Exercise detection statistics:")
                    for exercise, count in exercise_detections.items():
                        info = analyzer.exercise_info.get(exercise, {})
                        emoji = info.get('emoji', 'ğŸ‹ï¸')
                        name_display = info.get('name_display', exercise)
                        percentage = (count / sum(exercise_detections.values())) * 100
                        print(f"  â€¢ {emoji} {name_display}: {count} frames ({percentage:.1f}%)")
                
                if args.output:
                    print(f"\nğŸ’¾ Annotated video saved: {args.output}")
                
            else:
                print(f"âŒ Video analysis failed: {result.get('error', 'Unknown error')}")
                return 1
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ User interrupted")
        return 0
    except Exception as e:
        print(f"âŒ Execution error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())