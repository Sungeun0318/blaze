#!/usr/bin/env python3
"""
ğŸ¤– ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ê¸° - ì‚¬ì§„/ì˜ìƒ/ì‹¤ì‹œê°„ í†µí•© ë²„ì „
1ë‹¨ê³„: AIê°€ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€
2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ìƒì„¸ ê°ë„ ë¶„ì„
3ë‹¨ê³„: ìš´ë™ë³„ ë§ì¶¤ í”¼ë“œë°± + ì´ˆë¡/ë¹¨ê°• í™”ë©´ í‘œì‹œ
4ë‹¨ê³„: ì‚¬ì§„, ì˜ìƒ, ì‹¤ì‹œê°„ ëª¨ë‘ ì§€ì›
"""
import cv2
import numpy as np
import mediapipe as mp
import time
import argparse
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import deque
import json
from datetime import datetime
import tempfile



class CompleteAutoExerciseAnalyzer:
    """ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ê¸° - ì‚¬ì§„/ì˜ìƒ/ì‹¤ì‹œê°„ í†µí•©"""
    
    def __init__(self):
        # MediaPipe ì´ˆê¸°í™”
        self.mp_pose = mp.solutions.pose
        self.pose_static = self.mp_pose.Pose(
            static_image_mode=True,
            model_complexity=2,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.pose_video = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,
            enable_segmentation=False,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ
        self.exercise_classifier = None
        self.model_loaded = False
        self.temp_dir = tempfile.mkdtemp()
        self.load_exercise_model()
        
        # Enhanced ê°ë„ ê¸°ì¤€
        self.exercise_thresholds = {
            'squat': {
                'left_knee': {'points': [23, 25, 27], 'range': (55, 140), 'weight': 1.1, 'name_kr': 'ì™¼ìª½ ë¬´ë¦'},
                'right_knee': {'points': [24, 26, 28], 'range': (55, 140), 'weight': 1.1, 'name_kr': 'ì˜¤ë¥¸ìª½ ë¬´ë¦'},
                'left_hip': {'points': [11, 23, 25], 'range': (55, 140), 'weight': 0.9, 'name_kr': 'ì™¼ìª½ ì—‰ë©ì´'},
                'right_hip': {'points': [12, 24, 26], 'range': (55, 140), 'weight': 0.9, 'name_kr': 'ì˜¤ë¥¸ìª½ ì—‰ë©ì´'},
                'back_straight': {'points': [11, 23, 25], 'range': (110, 170), 'weight': 1.1, 'name_kr': 'ë“± ê³§ê²Œ'},
                'spine_angle': {'points': [23, 11, 13], 'range': (110, 170), 'weight': 0.9, 'name_kr': 'ì²™ì¶” ê°ë„'},
            },
            'push_up': {
                'left_elbow': {'points': [11, 13, 15], 'range': (40, 160), 'weight': 1.0, 'name_kr': 'ì™¼ìª½ íŒ”ê¿ˆì¹˜'},
                'right_elbow': {'points': [12, 14, 16], 'range': (40, 160), 'weight': 1.0, 'name_kr': 'ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜'},
                'body_line': {'points': [11, 23, 25], 'range': (140, 180), 'weight': 1.2, 'name_kr': 'ëª¸ ì¼ì§ì„ '},
                'leg_straight': {'points': [23, 25, 27], 'range': (140, 180), 'weight': 0.8, 'name_kr': 'ë‹¤ë¦¬ í´ê¸°'},
                'shoulder_alignment': {'points': [13, 11, 23], 'range': (120, 180), 'weight': 0.6, 'name_kr': 'ì–´ê¹¨ ì •ë ¬'},
                'core_stability': {'points': [11, 12, 23], 'range': (140, 180), 'weight': 1.0, 'name_kr': 'ì½”ì–´ ì•ˆì •ì„±'},
            },
            'deadlift': {
                'left_knee': {'points': [23, 25, 27], 'range': (80, 140), 'weight': 0.6, 'name_kr': 'ì™¼ìª½ ë¬´ë¦'},
                'right_knee': {'points': [24, 26, 28], 'range': (80, 140), 'weight': 0.6, 'name_kr': 'ì˜¤ë¥¸ìª½ ë¬´ë¦'},
                'hip_hinge': {'points': [11, 23, 25], 'range': (80, 180), 'weight': 0.7, 'name_kr': 'í™ íŒì§€'},
                'back_straight': {'points': [11, 23, 12], 'range': (120, 180), 'weight': 1.0, 'name_kr': 'ë“± ê³§ê²Œ'},
                'chest_up': {'points': [23, 11, 13], 'range': (50, 140), 'weight': 0.5, 'name_kr': 'ê°€ìŠ´ í´ê¸°'},
                'spine_neutral': {'points': [23, 11, 24], 'range': (120, 180), 'weight': 0.8, 'name_kr': 'ì²™ì¶” ì¤‘ë¦½'},
            },
            'bench_press': {
                'left_elbow': {'points': [11, 13, 15], 'range': (50, 145), 'weight': 1.1, 'name_kr': 'ì™¼ìª½ íŒ”ê¿ˆì¹˜'},
                'right_elbow': {'points': [12, 14, 16], 'range': (50, 145), 'weight': 1.1, 'name_kr': 'ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜'},
                'left_shoulder': {'points': [13, 11, 23], 'range': (50, 150), 'weight': 0.9, 'name_kr': 'ì™¼ìª½ ì–´ê¹¨'},
                'right_shoulder': {'points': [14, 12, 24], 'range': (50, 150), 'weight': 0.9, 'name_kr': 'ì˜¤ë¥¸ìª½ ì–´ê¹¨'},
                'back_arch': {'points': [11, 23, 25], 'range': (90, 170), 'weight': 0.7, 'name_kr': 'ë“± ì•„ì¹˜'},
                'wrist_alignment': {'points': [13, 15, 17], 'range': (70, 180), 'weight': 0.6, 'name_kr': 'ì†ëª© ì •ë ¬'},
            },
            'lunge': {
                'front_knee': {'points': [23, 25, 27], 'range': (70, 120), 'weight': 1.2, 'name_kr': 'ì• ë¬´ë¦'},
                'back_knee': {'points': [24, 26, 28], 'range': (120, 180), 'weight': 1.0, 'name_kr': 'ë’¤ ë¬´ë¦'},
                'front_hip': {'points': [11, 23, 25], 'range': (70, 120), 'weight': 0.8, 'name_kr': 'ì• ì—‰ë©ì´'},
                'torso_upright': {'points': [11, 23, 25], 'range': (100, 180), 'weight': 1.2, 'name_kr': 'ìƒì²´ ì§ë¦½'},
                'front_ankle': {'points': [25, 27, 31], 'range': (80, 110), 'weight': 0.8, 'name_kr': 'ì• ë°œëª©'},
                'back_hip_extension': {'points': [12, 24, 26], 'range': (150, 180), 'weight': 1.0, 'name_kr': 'ë’¤ ì—‰ë©ì´ ì‹ ì „'},
            }
        }
        
        # ìš´ë™ë³„ ë§ì¶¤ í”¼ë“œë°± ë©”ì‹œì§€
        self.detailed_feedback = {
            'squat': {
                'left_knee': {
                    'too_low': 'ì™¼ìª½ ë¬´ë¦ì„ ë” ì˜¬ë ¤ì£¼ì„¸ìš” (ë¬´ë¦ì´ ë„ˆë¬´ êµ¬ë¶€ëŸ¬ì ¸ ìˆì–´ìš”)',
                    'too_high': 'ì™¼ìª½ ë¬´ë¦ì„ ë” êµ¬ë¶€ë ¤ì£¼ì„¸ìš” (ìŠ¤ì¿¼íŠ¸ ê¹Šì´ê°€ ë¶€ì¡±í•´ìš”)',
                    'good': 'ì™¼ìª½ ë¬´ë¦ ê°ë„ê°€ ì™„ë²½í•´ìš”!'
                },
                'right_knee': {
                    'too_low': 'ì˜¤ë¥¸ìª½ ë¬´ë¦ì„ ë” ì˜¬ë ¤ì£¼ì„¸ìš” (ë¬´ë¦ì´ ë„ˆë¬´ êµ¬ë¶€ëŸ¬ì ¸ ìˆì–´ìš”)',
                    'too_high': 'ì˜¤ë¥¸ìª½ ë¬´ë¦ì„ ë” êµ¬ë¶€ë ¤ì£¼ì„¸ìš” (ìŠ¤ì¿¼íŠ¸ ê¹Šì´ê°€ ë¶€ì¡±í•´ìš”)',
                    'good': 'ì˜¤ë¥¸ìª½ ë¬´ë¦ ê°ë„ê°€ ì™„ë²½í•´ìš”!'
                },
                'left_hip': {
                    'too_low': 'ì™¼ìª½ ì—‰ë©ì´ë¥¼ ë” ë’¤ë¡œ ë¹¼ì£¼ì„¸ìš”',
                    'too_high': 'ì™¼ìª½ ì—‰ë©ì´ë¥¼ ë” ë‚®ì¶°ì£¼ì„¸ìš”',
                    'good': 'ì™¼ìª½ ì—‰ë©ì´ ìì„¸ê°€ ì¢‹ì•„ìš”!'
                },
                'right_hip': {
                    'too_low': 'ì˜¤ë¥¸ìª½ ì—‰ë©ì´ë¥¼ ë” ë’¤ë¡œ ë¹¼ì£¼ì„¸ìš”',
                    'too_high': 'ì˜¤ë¥¸ìª½ ì—‰ë©ì´ë¥¼ ë” ë‚®ì¶°ì£¼ì„¸ìš”',
                    'good': 'ì˜¤ë¥¸ìª½ ì—‰ë©ì´ ìì„¸ê°€ ì¢‹ì•„ìš”!'
                },
                'back_straight': {
                    'too_low': 'ë“±ì„ ë” ê³§ê²Œ í´ì£¼ì„¸ìš” (ë“±ì´ êµ½ì–´ìˆì–´ìš”)',
                    'too_high': 'ìƒì²´ë¥¼ ì•½ê°„ ì•ìœ¼ë¡œ ê¸°ìš¸ì—¬ì£¼ì„¸ìš”',
                    'good': 'ë“±ì´ ì™„ë²½í•˜ê²Œ ê³§ì•„ìš”!'
                },
                'general': 'ë¬´ë¦ì´ ë°œëì„ ë„˜ì§€ ì•Šê²Œ ì£¼ì˜í•˜ì„¸ìš”'
            },
            'push_up': {
                'left_elbow': {
                    'too_low': 'ì™¼ìª½ íŒ”ì„ ë” í´ì£¼ì„¸ìš”',
                    'too_high': 'ì™¼ìª½ íŒ”ê¿ˆì¹˜ë¥¼ ë” êµ¬ë¶€ë ¤ì£¼ì„¸ìš”',
                    'good': 'ì™¼ìª½ íŒ” ê°ë„ê°€ ì™„ë²½í•´ìš”!'
                },
                'right_elbow': {
                    'too_low': 'ì˜¤ë¥¸ìª½ íŒ”ì„ ë” í´ì£¼ì„¸ìš”',
                    'too_high': 'ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜ë¥¼ ë” êµ¬ë¶€ë ¤ì£¼ì„¸ìš”',
                    'good': 'ì˜¤ë¥¸ìª½ íŒ” ê°ë„ê°€ ì™„ë²½í•´ìš”!'
                },
                'body_line': {
                    'too_low': 'ì—‰ë©ì´ë¥¼ ì˜¬ë ¤ì£¼ì„¸ìš” (ëª¸ì´ êµ¬ë¶€ëŸ¬ì ¸ ìˆì–´ìš”)',
                    'too_high': 'ì—‰ë©ì´ë¥¼ ë‚´ë ¤ì£¼ì„¸ìš” (ì—‰ë©ì´ê°€ ë„ˆë¬´ ë†’ì•„ìš”)',
                    'good': 'ëª¸ì´ ì™„ë²½í•œ ì¼ì§ì„ ì´ì—ìš”!'
                },
                'shoulder_alignment': {
                    'too_low': 'ì–´ê¹¨ë¥¼ ë” ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•˜ì„¸ìš”',
                    'too_high': 'ì–´ê¹¨ì— í˜ì„ ë¹¼ê³  ìì—°ìŠ¤ëŸ½ê²Œ í•˜ì„¸ìš”',
                    'good': 'ì–´ê¹¨ ì •ë ¬ì´ ì™„ë²½í•´ìš”!'
                },
                'general': 'íŒ”ê¿ˆì¹˜ë¥¼ ëª¸ì— ê°€ê¹ê²Œ ìœ ì§€í•˜ì„¸ìš”'
            },
            'deadlift': {
                'left_knee': {
                    'too_low': 'ì™¼ìª½ ë¬´ë¦ì„ ì•½ê°„ ë” í´ì£¼ì„¸ìš”',
                    'too_high': 'ì™¼ìª½ ë¬´ë¦ì„ ì•½ê°„ êµ¬ë¶€ë ¤ì£¼ì„¸ìš”',
                    'good': 'ì™¼ìª½ ë¬´ë¦ì´ ì™„ë²½í•´ìš”!'
                },
                'right_knee': {
                    'too_low': 'ì˜¤ë¥¸ìª½ ë¬´ë¦ì„ ì•½ê°„ ë” í´ì£¼ì„¸ìš”',
                    'too_high': 'ì˜¤ë¥¸ìª½ ë¬´ë¦ì„ ì•½ê°„ êµ¬ë¶€ë ¤ì£¼ì„¸ìš”',
                    'good': 'ì˜¤ë¥¸ìª½ ë¬´ë¦ì´ ì™„ë²½í•´ìš”!'
                },
                'hip_hinge': {
                    'too_low': 'ì—‰ë©ì´ë¥¼ ë” ë’¤ë¡œ ë¹¼ì£¼ì„¸ìš” (í™ íŒì§€ ë™ì‘)',
                    'too_high': 'ì—‰ë©ì´ë¥¼ ë” ë‚®ì¶°ì£¼ì„¸ìš”',
                    'good': 'í™ íŒì§€ ë™ì‘ì´ ì™„ë²½í•´ìš”!'
                },
                'back_straight': {
                    'too_low': 'ë“±ì„ ê³§ê²Œ í´ì£¼ì„¸ìš” - ë§¤ìš° ì¤‘ìš”í•´ìš”!',
                    'too_high': 'ë“±ì— í˜ì„ ë¹¼ê³  ìì—°ìŠ¤ëŸ½ê²Œ í•˜ì„¸ìš”',
                    'good': 'ë“±ì´ ì™„ë²½í•˜ê²Œ ê³§ì•„ìš”!'
                },
                'chest_up': {
                    'too_low': 'ê°€ìŠ´ì„ í´ê³  ì‹œì„ ì„ ì•ìœ¼ë¡œ í–¥í•˜ì„¸ìš”',
                    'too_high': 'ê³¼ë„í•˜ê²Œ ê°€ìŠ´ì„ í´ì§€ ë§ˆì„¸ìš”',
                    'good': 'ê°€ìŠ´ ìì„¸ê°€ ì™„ë²½í•´ìš”!'
                },
                'general': 'ë°”ë²¨ì„ ëª¸ì— ê°€ê¹ê²Œ ìœ ì§€í•˜ì„¸ìš”'
            },
            'bench_press': {
                'left_elbow': {
                    'too_low': 'ì™¼ìª½ íŒ”ì„ ë” í´ì£¼ì„¸ìš”',
                    'too_high': 'ì™¼ìª½ íŒ”ê¿ˆì¹˜ë¥¼ ë” êµ¬ë¶€ë ¤ì£¼ì„¸ìš”',
                    'good': 'ì™¼ìª½ íŒ”ì´ ì™„ë²½í•´ìš”!'
                },
                'right_elbow': {
                    'too_low': 'ì˜¤ë¥¸ìª½ íŒ”ì„ ë” í´ì£¼ì„¸ìš”',
                    'too_high': 'ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜ë¥¼ ë” êµ¬ë¶€ë ¤ì£¼ì„¸ìš”',
                    'good': 'ì˜¤ë¥¸ìª½ íŒ”ì´ ì™„ë²½í•´ìš”!'
                },
                'left_shoulder': {
                    'too_low': 'ì™¼ìª½ ì–´ê¹¨ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•˜ì„¸ìš”',
                    'too_high': 'ì™¼ìª½ ì–´ê¹¨ì— í˜ì„ ë¹¼ì„¸ìš”',
                    'good': 'ì™¼ìª½ ì–´ê¹¨ê°€ ì™„ë²½í•´ìš”!'
                },
                'right_shoulder': {
                    'too_low': 'ì˜¤ë¥¸ìª½ ì–´ê¹¨ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•˜ì„¸ìš”',
                    'too_high': 'ì˜¤ë¥¸ìª½ ì–´ê¹¨ì— í˜ì„ ë¹¼ì„¸ìš”',
                    'good': 'ì˜¤ë¥¸ìª½ ì–´ê¹¨ê°€ ì™„ë²½í•´ìš”!'
                },
                'back_arch': {
                    'too_low': 'ìì—°ìŠ¤ëŸ¬ìš´ ë“± ì•„ì¹˜ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”',
                    'too_high': 'ë“± ì•„ì¹˜ë¥¼ ê³¼ë„í•˜ê²Œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”',
                    'good': 'ë“± ì•„ì¹˜ê°€ ì™„ë²½í•´ìš”!'
                },
                'general': 'ë°”ë²¨ì„ ì²œì²œíˆ ì»¨íŠ¸ë¡¤í•˜ì„¸ìš”'
            },
            'lunge': {
                'front_knee': {
                    'too_low': 'ì• ë¬´ë¦ì„ 90ë„ë¡œ ë§ì¶°ì£¼ì„¸ìš” (ë„ˆë¬´ êµ¬ë¶€ëŸ¬ì ¸ ìˆì–´ìš”)',
                    'too_high': 'ì• ë¬´ë¦ì„ ë” êµ¬ë¶€ë ¤ì£¼ì„¸ìš” (90ë„ê¹Œì§€)',
                    'good': 'ì• ë¬´ë¦ì´ ì™„ë²½í•œ 90ë„ì—ìš”!'
                },
                'back_knee': {
                    'too_low': 'ë’¤ ë¬´ë¦ì„ ë” í´ì£¼ì„¸ìš”',
                    'too_high': 'ë’¤ ë¬´ë¦ì´ ì™„ë²½í•´ìš”!',
                    'good': 'ë’¤ ë¬´ë¦ì´ ì™„ë²½í•˜ê²Œ í´ì ¸ ìˆì–´ìš”!'
                },
                'torso_upright': {
                    'too_low': 'ìƒì²´ë¥¼ ë” ê³§ê²Œ ì„¸ì›Œì£¼ì„¸ìš”',
                    'too_high': 'ìƒì²´ê°€ ì™„ë²½í•´ìš”!',
                    'good': 'ìƒì²´ê°€ ì™„ë²½í•˜ê²Œ ì§ë¦½í•´ìš”!'
                },
                'front_ankle': {
                    'too_low': 'ì•ë°œëª©ì„ ë” ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€í•˜ì„¸ìš”',
                    'too_high': 'ì•ë°œëª©ì— í˜ì„ ë¹¼ì„¸ìš”',
                    'good': 'ì•ë°œëª©ì´ ì™„ë²½í•´ìš”!'
                },
                'general': 'ê· í˜•ì„ ìœ ì§€í•˜ë©° ì²œì²œíˆ ë™ì‘í•˜ì„¸ìš”'
            }
        }
        
        # Enhanced ë¶„ë¥˜ ì„ê³„ê°’
        self.classification_thresholds = {
            'squat': 0.5,
            'push_up': 0.7,
            'deadlift': 0.8,  # ì™„í™”
            'bench_press': 0.5,
            'lunge': 0.6,
        }
        
        # ìš´ë™ ì´ëª¨ì§€ ë° í•œê¸€ëª…
        self.exercise_info = {
            'squat': {'emoji': 'ğŸ‹ï¸â€â™€ï¸', 'name_kr': 'ìŠ¤ì¿¼íŠ¸', 'name_en': 'SQUAT'},
            'push_up': {'emoji': 'ğŸ’ª', 'name_kr': 'í‘¸ì‰¬ì—…', 'name_en': 'PUSH-UP'},
            'deadlift': {'emoji': 'ğŸ‹ï¸â€â™‚ï¸', 'name_kr': 'ë°ë“œë¦¬í”„íŠ¸', 'name_en': 'DEADLIFT'},
            'bench_press': {'emoji': 'ğŸ”¥', 'name_kr': 'ë²¤ì¹˜í”„ë ˆìŠ¤', 'name_en': 'BENCH PRESS'},
            'lunge': {'emoji': 'ğŸš€', 'name_kr': 'ëŸ°ì§€', 'name_en': 'LUNGE'}
        }
        
        # ìƒíƒœ ê´€ë¦¬
        self.current_exercise = "detecting..."
        self.current_pose_quality = "unknown"
        self.exercise_confidence = 0.0
        self.pose_confidence = 0.0
        
        # ì•ˆì •í™”ë¥¼ ìœ„í•œ íˆìŠ¤í† ë¦¬
        self.exercise_history = deque(maxlen=10)
        self.pose_history = deque(maxlen=5)
        
        # í†µê³„
        self.stats = {'good': 0, 'bad': 0, 'frames': 0}
        
        # í™”ë©´ ìƒíƒœ (ë¶€ë“œëŸ¬ìš´ ì „í™˜)
        self.screen_color = (128, 128, 128)  # ê¸°ë³¸ íšŒìƒ‰
        self.target_color = (128, 128, 128)
        self.color_transition_speed = 0.15
        
        # íƒ€ì´ë°
        self.last_classification_time = 0
        self.classification_interval = 2.0  # 2ì´ˆë§ˆë‹¤ ìš´ë™ ë¶„ë¥˜
        
        # í”¼ë“œë°± ë©”ì‹œì§€ ê´€ë¦¬
        self.current_feedback_messages = []
        self.last_feedback_time = 0
        self.feedback_interval = 1.0  # 1ì´ˆë§ˆë‹¤ í”¼ë“œë°± ì—…ë°ì´íŠ¸
    
    def load_exercise_model(self):
        """AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ"""
        model_path = "models/exercise_classifier.pkl"
        try:
            if os.path.exists(model_path):
                from exercise_classifier import ExerciseClassificationModel
                self.exercise_classifier = ExerciseClassificationModel()
                self.model_loaded = self.exercise_classifier.load_model(model_path)
                if self.model_loaded:
                    print("âœ… AI ìš´ë™ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                else:
                    print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            else:
                print("âš ï¸ AI ëª¨ë¸ ì—†ìŒ - ìˆ˜ë™ ìš´ë™ ì„ íƒ ëª¨ë“œ")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.model_loaded = False
    
    def calculate_angle(self, p1: Tuple[float, float], p2: Tuple[float, float], p3: Tuple[float, float]) -> float:
        """ê°ë„ ê³„ì‚°"""
        try:
            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]], dtype=np.float64)
            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]], dtype=np.float64)
            
            v1_mag = np.linalg.norm(v1)
            v2_mag = np.linalg.norm(v2)
            
            if v1_mag < 1e-6 or v2_mag < 1e-6:
                return 180.0
            
            cos_angle = np.dot(v1, v2) / (v1_mag * v2_mag)
            cos_angle = np.clip(cos_angle, -1.0, 1.0)
            angle = np.degrees(np.arccos(cos_angle))
            
            return float(angle)
        except:
            return 180.0
    
    def classify_exercise(self, frame: np.ndarray) -> Tuple[str, float]:
        """ğŸ¤– 1ë‹¨ê³„: AIë¡œ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€"""
        current_time = time.time()
        
        # ë¶„ë¥˜ ì£¼ê¸° ì œì–´ (2ì´ˆë§ˆë‹¤)
        if current_time - self.last_classification_time < self.classification_interval:
            return self.current_exercise, self.exercise_confidence
        
        if not self.model_loaded:
            return "manual_mode", 0.0
        
        try:
            # ì„ì‹œ ì´ë¯¸ì§€ ì €ì¥
            temp_path = os.path.join(self.temp_dir, "temp_frame.jpg")
            cv2.imwrite(temp_path, frame)
            
            # AI ìš´ë™ ë¶„ë¥˜
            exercise, confidence = self.exercise_classifier.predict(temp_path)
            
            # íˆìŠ¤í† ë¦¬ ì•ˆì •í™”
            self.exercise_history.append((exercise, confidence))
            
            if len(self.exercise_history) >= 3:
                # ìµœê·¼ 3ê°œ ê²°ê³¼ì˜ í•©ì˜
                recent = list(self.exercise_history)[-3:]
                high_conf_predictions = [(ex, conf) for ex, conf in recent if conf > 0.6]
                
                if high_conf_predictions:
                    from collections import Counter
                    exercises = [ex for ex, conf in high_conf_predictions]
                    most_common = Counter(exercises).most_common(1)[0]
                    
                    if most_common[1] >= 2:  # 2ë²ˆ ì´ìƒ ê°ì§€
                        new_exercise = most_common[0]
                        if new_exercise != self.current_exercise:
                            self.current_exercise = new_exercise
                            self.exercise_confidence = confidence
                            exercise_info = self.exercise_info.get(new_exercise, {})
                            emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                            name_kr = exercise_info.get('name_kr', new_exercise)
                            print(f"ğŸ¤– AI ê°ì§€: {emoji} {name_kr} (ì‹ ë¢°ë„: {confidence:.1%})")
            
            self.last_classification_time = current_time
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            return self.current_exercise, self.exercise_confidence
            
        except Exception as e:
            print(f"ìš´ë™ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            return self.current_exercise, self.exercise_confidence
    
    def analyze_pose_angles(self, landmarks, exercise: str) -> Dict:
        """ğŸ¯ 2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ìƒì„¸ ê°ë„ ë¶„ì„"""
        if exercise not in self.exercise_thresholds:
            return {'valid': False, 'error': f'ì§€ì›ë˜ì§€ ì•ŠëŠ” ìš´ë™: {exercise}'}
        
        thresholds = self.exercise_thresholds[exercise]
        angles = {}
        violations = []
        weighted_violation_score = 0.0
        total_weight = 0.0
        
        for joint_name, config in thresholds.items():
            try:
                p1_idx, p2_idx, p3_idx = config['points']
                min_angle, max_angle = config['range']
                weight = config['weight']
                
                # ê°€ì‹œì„± í™•ì¸
                if (landmarks[p1_idx].visibility < 0.25 or 
                    landmarks[p2_idx].visibility < 0.25 or 
                    landmarks[p3_idx].visibility < 0.25):
                    continue
                
                p1 = (landmarks[p1_idx].x, landmarks[p1_idx].y)
                p2 = (landmarks[p2_idx].x, landmarks[p2_idx].y)
                p3 = (landmarks[p3_idx].x, landmarks[p3_idx].y)
                
                angle = self.calculate_angle(p1, p2, p3)
                angles[joint_name] = {
                    'value': angle,
                    'range': (min_angle, max_angle),
                    'weight': weight,
                    'in_range': min_angle <= angle <= max_angle,
                    'name_kr': config.get('name_kr', joint_name)
                }
                
                total_weight += weight
                
                if not (min_angle <= angle <= max_angle):
                    violations.append({
                        'joint': joint_name,
                        'angle': angle,
                        'expected_range': (min_angle, max_angle),
                        'weight': weight,
                        'name_kr': config.get('name_kr', joint_name)
                    })
                    weighted_violation_score += weight
                    
            except Exception as e:
                continue
        
        # Enhanced ë¶„ë¥˜
        violation_ratio = weighted_violation_score / max(total_weight, 1.0)
        classification_threshold = self.classification_thresholds.get(exercise, 0.6)
        is_good = violation_ratio < classification_threshold
        
        return {
            'valid': True,
            'classification': 'good' if is_good else 'bad',
            'confidence': 1.0 - violation_ratio,
            'angles': angles,
            'violations': violations,
            'violation_ratio': violation_ratio,
            'threshold': classification_threshold
        }
    
    def generate_detailed_feedback(self, exercise: str, pose_result: Dict) -> List[str]:
        """ğŸ—£ï¸ ìš´ë™ë³„ ìƒì„¸ í”¼ë“œë°± ìƒì„±"""
        current_time = time.time()
        
        # í”¼ë“œë°± ì£¼ê¸° ì œí•œ
        if current_time - self.last_feedback_time < self.feedback_interval:
            return self.current_feedback_messages
        
        messages = []
        
        if not pose_result.get('valid', False):
            messages.append("í¬ì¦ˆë¥¼ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return messages
        
        violations = pose_result.get('violations', [])
        exercise_feedback = self.detailed_feedback.get(exercise, {})
        
        if not violations:
            # ëª¨ë“  ìì„¸ê°€ ì™„ë²½í•œ ê²½ìš°
            exercise_info = self.exercise_info.get(exercise, {})
            name_kr = exercise_info.get('name_kr', exercise)
            messages.append(f"ì™„ë²½í•œ {name_kr} ìì„¸ì…ë‹ˆë‹¤! ğŸ‘")
            messages.append("í˜„ì¬ í¼ì„ ìœ ì§€í•˜ì„¸ìš”!")
        else:
            # ìœ„ë°˜ì‚¬í•­ì´ ìˆëŠ” ê²½ìš° - ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
            violations_sorted = sorted(violations, key=lambda x: x['weight'], reverse=True)
            
            for i, violation in enumerate(violations_sorted[:3]):  # ìƒìœ„ 3ê°œë§Œ
                joint = violation['joint']
                angle = violation['angle']
                min_angle, max_angle = violation['expected_range']
                name_kr = violation.get('name_kr', joint)
                
                joint_feedback = exercise_feedback.get(joint, {})
                
                if angle < min_angle:
                    # ê°ë„ê°€ ë„ˆë¬´ ì‘ìŒ
                    message = joint_feedback.get('too_low', f'{name_kr} ê°ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”')
                elif angle > max_angle:
                    # ê°ë„ê°€ ë„ˆë¬´ í¼
                    message = joint_feedback.get('too_high', f'{name_kr} ê°ë„ë¥¼ ë‚®ì¶°ì£¼ì„¸ìš”')
                else:
                    message = joint_feedback.get('good', f'{name_kr}ê°€ ì¢‹ì•„ìš”!')
                
                messages.append(f"âš ï¸ {message}")
                
                # êµ¬ì²´ì ì¸ ê°ë„ ì •ë³´ ì¶”ê°€
                if i == 0:  # ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ì œë§Œ ê°ë„ í‘œì‹œ
                    messages.append(f"   í˜„ì¬: {angle:.0f}Â° â†’ ëª©í‘œ: {min_angle:.0f}-{max_angle:.0f}Â°")
            
            # ì¼ë°˜ì ì¸ ìš´ë™ë³„ ì¡°ì–¸ ì¶”ê°€
            general_advice = exercise_feedback.get('general', '')
            if general_advice and len(violations_sorted) <= 2:
                messages.append(f"ğŸ’¡ {general_advice}")
        
        self.current_feedback_messages = messages
        self.last_feedback_time = current_time
        return messages
    
    def update_screen_color(self, pose_quality: str):
        """ğŸŒˆ ì´ˆë¡/ë¹¨ê°• í™”ë©´ ìƒ‰ìƒ ì—…ë°ì´íŠ¸"""
        if pose_quality == 'good':
            self.target_color = (0, 255, 0)      # ì´ˆë¡ìƒ‰
        elif pose_quality == 'bad':
            self.target_color = (0, 0, 255)      # ë¹¨ê°„ìƒ‰
        elif pose_quality == 'detecting':
            self.target_color = (255, 255, 0)    # ë…¸ë€ìƒ‰
        else:
            self.target_color = (128, 128, 128)  # íšŒìƒ‰
        
        # ë¶€ë“œëŸ¬ìš´ ìƒ‰ìƒ ì „í™˜
        for i in range(3):
            current = self.screen_color[i]
            target = self.target_color[i]
            diff = target - current
            self.screen_color = tuple(
                int(current + diff * self.color_transition_speed) if j == i 
                else self.screen_color[j] for j in range(3)
            )
    
    def draw_enhanced_overlay(self, frame: np.ndarray, exercise: str, pose_result: Dict) -> np.ndarray:
        """âœ¨ í–¥ìƒëœ ë¶„ì„ ê²°ê³¼ í™”ë©´ ì˜¤ë²„ë ˆì´"""
        height, width = frame.shape[:2]
        
        # ğŸŒˆ ì „ì²´ í™”ë©´ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ ë° í…Œë‘ë¦¬
        if pose_result.get('valid', False):
            pose_quality = pose_result['classification']
            self.update_screen_color(pose_quality)
            
            # íˆ¬ëª…í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´
            overlay = frame.copy()
            cv2.rectangle(overlay, (0, 0), (width, height), self.screen_color, -1)
            cv2.addWeighted(overlay, 0.1, frame, 0.9, 0, frame)
        
        # ğŸ¯ ë‘êº¼ìš´ í…Œë‘ë¦¬
        border_thickness = 30
        cv2.rectangle(frame, (0, 0), (width, height), self.screen_color, border_thickness)
        
        # ğŸ“ ì™¼ìª½ ìœ„: ìš´ë™ ì¢…ë¥˜ í‘œì‹œ
        exercise_info = self.exercise_info.get(exercise, {})
        if exercise != "detecting..." and exercise != "manual_mode":
            emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
            name_kr = exercise_info.get('name_kr', exercise)
            name_en = exercise_info.get('name_en', exercise.upper())
            
            # ë°°ê²½ ë°•ìŠ¤
            cv2.rectangle(frame, (40, 40), (400, 140), (0, 0, 0), -1)
            cv2.rectangle(frame, (40, 40), (400, 140), self.screen_color, 3)
            
            # ìš´ë™ëª… í‘œì‹œ
            exercise_text = f"{emoji} {name_kr}"
            cv2.putText(frame, exercise_text, (60, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
            
            # ì˜ì–´ëª… í‘œì‹œ
            cv2.putText(frame, name_en, (60, 110), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (200, 200, 200), 2)
            
            # ì‹ ë¢°ë„ í‘œì‹œ
            confidence_text = f"ì‹ ë¢°ë„: {self.exercise_confidence:.0%}"
            cv2.putText(frame, confidence_text, (250, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)
            
        elif exercise == "detecting...":
            cv2.rectangle(frame, (40, 40), (300, 100), (0, 0, 0), -1)
            cv2.rectangle(frame, (40, 40), (300, 100), (255, 255, 0), 3)
            cv2.putText(frame, "ğŸ¤– ìš´ë™ ê°ì§€ ì¤‘...", (60, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
        else:
            cv2.rectangle(frame, (40, 40), (350, 100), (0, 0, 0), -1)
            cv2.rectangle(frame, (40, 40), (350, 100), (128, 128, 128), 3)
            cv2.putText(frame, "âš™ï¸ ìˆ˜ë™ ëª¨ë“œ", (60, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ğŸ¯ ì¤‘ì•™ ìƒíƒœ ë©”ì‹œì§€
        if pose_result.get('valid', False):
            pose_quality = pose_result['classification']
            confidence = pose_result['confidence']
            
            if pose_quality == 'good':
                status_text = "ì™„ë²½í•œ ìì„¸! ğŸ‘"
                status_color = (0, 255, 0)
            else:
                status_text = "ìì„¸ êµì • í•„ìš” âš ï¸"
                status_color = (0, 0, 255)
            
            # ì¤‘ì•™ ìƒíƒœ í‘œì‹œ
            status_size = cv2.getTextSize(status_text, cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)[0]
            status_x = (width - status_size[0]) // 2
            status_y = height // 2 - 80
            
            cv2.rectangle(frame, (status_x - 30, status_y - 40), 
                         (status_x + status_size[0] + 30, status_y + 20), (0, 0, 0), -1)
            cv2.rectangle(frame, (status_x - 30, status_y - 40), 
                         (status_x + status_size[0] + 30, status_y + 20), status_color, 4)
            
            cv2.putText(frame, status_text, (status_x, status_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, status_color, 3)
            
            # ì‹ ë¢°ë„ ì ìˆ˜
            score_text = f"ìì„¸ ì ìˆ˜: {confidence:.0%}"
            score_size = cv2.getTextSize(score_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]
            score_x = (width - score_size[0]) // 2
            cv2.putText(frame, score_text, (score_x, status_y + 50), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ğŸ“ ì™¼ìª½ ì•„ë˜: ìƒì„¸ í”¼ë“œë°± ë©”ì‹œì§€
        if exercise in self.exercise_thresholds:
            feedback_messages = self.generate_detailed_feedback(exercise, pose_result)
            
            if feedback_messages:
                # í”¼ë“œë°± ì˜ì—­ ë°°ê²½
                feedback_height = len(feedback_messages) * 35 + 60
                cv2.rectangle(frame, (40, height - feedback_height - 40), 
                             (width - 40, height - 40), (0, 0, 0), -1)
                cv2.rectangle(frame, (40, height - feedback_height - 40), 
                             (width - 40, height - 40), self.screen_color, 3)
                
                # í”¼ë“œë°± ì œëª©
                cv2.putText(frame, "ğŸ’¬ ì‹¤ì‹œê°„ í”¼ë“œë°±:", (60, height - feedback_height - 10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # í”¼ë“œë°± ë©”ì‹œì§€ë“¤
                for i, message in enumerate(feedback_messages[:5]):  # ìµœëŒ€ 5ê°œ
                    y_pos = height - feedback_height + 20 + (i * 35)
                    
                    # ë©”ì‹œì§€ ìƒ‰ìƒ ê²°ì •
                    if "ì™„ë²½" in message or "ğŸ‘" in message:
                        msg_color = (0, 255, 0)  # ì´ˆë¡ìƒ‰
                    elif "âš ï¸" in message:
                        msg_color = (0, 100, 255)  # ì£¼í™©ìƒ‰
                    elif "ğŸ’¡" in message:
                        msg_color = (255, 255, 0)  # ë…¸ë€ìƒ‰
                    else:
                        msg_color = (255, 255, 255)  # í°ìƒ‰
                    
                    cv2.putText(frame, message, (60, y_pos), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, msg_color, 2)
        
        # ğŸ“Š ì˜¤ë¥¸ìª½ ìœ„: í†µê³„ ì •ë³´
        if self.stats['frames'] > 0:
            total = self.stats['good'] + self.stats['bad']
            if total > 0:
                good_ratio = self.stats['good'] / total
                
                # í†µê³„ ë°°ê²½
                cv2.rectangle(frame, (width - 300, 40), (width - 40, 140), (0, 0, 0), -1)
                cv2.rectangle(frame, (width - 300, 40), (width - 40, 140), (255, 255, 255), 2)
                
                # í†µê³„ í…ìŠ¤íŠ¸
                cv2.putText(frame, "ğŸ“Š ìš´ë™ í†µê³„", (width - 280, 70), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                stats_text = f"Good: {self.stats['good']} | Bad: {self.stats['bad']}"
                cv2.putText(frame, stats_text, (width - 280, 95), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                ratio_text = f"ì„±ê³µë¥ : {good_ratio:.1%}"
                cv2.putText(frame, ratio_text, (width - 280, 115), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0) if good_ratio > 0.7 else (255, 255, 255), 1)
        
        # âŒ¨ï¸ í•˜ë‹¨ ì¡°ì‘ ê°€ì´ë“œ
        guide_text = "Q: ì¢…ë£Œ  |  R: ë¦¬ì…‹  |  S: ìŠ¤í¬ë¦°ìƒ·  |  C: ìš´ë™ ë³€ê²½  |  SPACE: ëª¨ë“œ ë³€ê²½"
        cv2.putText(frame, guide_text, (50, height - 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180, 180, 180), 1)
        
        return frame
    
    def analyze_single_image(self, image_path: str) -> Dict:
        """ğŸ“· ë‹¨ì¼ ì´ë¯¸ì§€ ì™„ì „ ìë™ ë¶„ì„"""
        if not os.path.exists(image_path):
            return {'error': f'ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}'}
        
        print(f"ğŸ“· ì´ë¯¸ì§€ ìë™ ë¶„ì„ ì‹œì‘: {os.path.basename(image_path)}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            return {'error': 'ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
        
        # í¬ì¦ˆ ê²€ì¶œ (ì •ì  ì´ë¯¸ì§€ìš© ê³ ì •ë°€ ëª¨ë¸)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.pose_static.process(image_rgb)
        
        if not results.pose_landmarks:
            return {'error': 'í¬ì¦ˆë¥¼ ê²€ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
        
        # ğŸ¤– 1ë‹¨ê³„: AI ìš´ë™ ê°ì§€
        exercise, confidence = self.classify_exercise(image)
        exercise_info = self.exercise_info.get(exercise, {})
        emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
        name_kr = exercise_info.get('name_kr', exercise)
        
        print(f"ğŸ¯ AI ê°ì§€: {emoji} {name_kr} (ì‹ ë¢°ë„: {confidence:.1%})")
        
        # ğŸ¯ 2ë‹¨ê³„: ê°ë„ ë¶„ì„
        if exercise in self.exercise_thresholds:
            pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, exercise)
            
            # ğŸ—£ï¸ 3ë‹¨ê³„: ìƒì„¸ í”¼ë“œë°± ìƒì„±
            feedback_messages = self.generate_detailed_feedback(exercise, pose_result)
            
            # ğŸ“¸ 4ë‹¨ê³„: ì£¼ì„ ì´ë¯¸ì§€ ìƒì„±
            annotated_image = image.copy()
            
            # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
            self.mp_drawing.draw_landmarks(
                annotated_image, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,
                landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
            
            # ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°
            annotated_image = self.draw_enhanced_overlay(annotated_image, exercise, pose_result)
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            return {
                'success': True,
                'image_path': image_path,
                'detected_exercise': exercise,
                'exercise_info': exercise_info,
                'exercise_confidence': confidence,
                'pose_analysis': pose_result,
                'feedback_messages': feedback_messages,
                'original_image': image,
                'annotated_image': annotated_image,
                'analysis_timestamp': datetime.now().isoformat()
            }
        else:
            return {'error': f'ì§€ì›ë˜ì§€ ì•ŠëŠ” ìš´ë™: {exercise}'}
    
    def analyze_video_file(self, video_path: str, output_path: str = None) -> Dict:
        """ğŸ¬ ì˜ìƒ íŒŒì¼ ì™„ì „ ìë™ ë¶„ì„"""
        if not os.path.exists(video_path):
            return {'error': f'ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}'}
        
        print(f"ğŸ¬ ì˜ìƒ ìë™ ë¶„ì„ ì‹œì‘: {os.path.basename(video_path)}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return {'error': 'ì˜ìƒ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì˜ìƒ ì •ë³´
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“¹ ì˜ìƒ ì •ë³´: {width}x{height}, {fps}fps, {total_frames}í”„ë ˆì„")
        
        # ì¶œë ¥ ì˜ìƒ ì„¤ì •
        if output_path:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        frame_results = []
        exercise_detections = {}
        stats = {'good': 0, 'bad': 0, 'total': 0}
        
        # ì„ì‹œë¡œ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        self.exercise_history.clear()
        current_exercise = "detecting..."
        
        try:
            frame_count = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # RGB ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # í¬ì¦ˆ ê²€ì¶œ
                results = self.pose_video.process(frame_rgb)
                
                if results.pose_landmarks:
                    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    self.mp_drawing.draw_landmarks(
                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
                    
                    # ğŸ¤– ìš´ë™ ê°ì§€ (ì˜ìƒìš©)
                    exercise, confidence = self.classify_exercise(frame)
                    
                    # ìš´ë™ ê°ì§€ í†µê³„
                    if exercise != "detecting..." and exercise != "manual_mode":
                        if exercise not in exercise_detections:
                            exercise_detections[exercise] = 0
                        exercise_detections[exercise] += 1
                        current_exercise = exercise
                    
                    # ğŸ¯ ê°ë„ ë¶„ì„
                    if current_exercise in self.exercise_thresholds:
                        pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, current_exercise)
                        
                        if pose_result['valid']:
                            pose_quality = pose_result['classification']
                            stats[pose_quality] += 1
                            stats['total'] += 1
                            
                            # ğŸ—£ï¸ í”¼ë“œë°± ìƒì„±
                            feedback_messages = self.generate_detailed_feedback(current_exercise, pose_result)
                            
                            # âœ¨ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°
                            frame = self.draw_enhanced_overlay(frame, current_exercise, pose_result)
                            
                            # ê²°ê³¼ ì €ì¥
                            frame_results.append({
                                'frame': frame_count,
                                'timestamp': frame_count / fps,
                                'exercise': current_exercise,
                                'classification': pose_quality,
                                'confidence': pose_result['confidence'],
                                'feedback': feedback_messages[:3]  # ìƒìœ„ 3ê°œë§Œ
                            })
                        else:
                            frame = self.draw_enhanced_overlay(frame, current_exercise, {'valid': False})
                    else:
                        frame = self.draw_enhanced_overlay(frame, current_exercise, {'valid': False})
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_count % (fps * 5) == 0:  # 5ì´ˆë§ˆë‹¤
                    progress = (frame_count / total_frames) * 100
                    print(f"ğŸ“Š ë¶„ì„ ì§„í–‰ë¥ : {progress:.1f}%")
                
                # ì¶œë ¥ ì˜ìƒì— ì“°ê¸°
                if output_path:
                    out.write(frame)
                
                frame_count += 1
                
        except Exception as e:
            print(f"âŒ ì˜ìƒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return {'error': f'ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}'}
        finally:
            cap.release()
            if output_path:
                out.release()
        
        # ê°€ì¥ ë§ì´ ê°ì§€ëœ ìš´ë™ ì°¾ê¸°
        main_exercise = max(exercise_detections.items(), key=lambda x: x[1])[0] if exercise_detections else "unknown"
        
        # ê²°ê³¼ ìš”ì•½
        success_rate = (stats['good'] / max(stats['total'], 1)) * 100
        
        print(f"\nğŸ‰ ì˜ìƒ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ¯ ì£¼ìš” ìš´ë™: {self.exercise_info.get(main_exercise, {}).get('name_kr', main_exercise)}")
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: Good {stats['good']}í”„ë ˆì„, Bad {stats['bad']}í”„ë ˆì„")
        print(f"ğŸ¯ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        return {
            'success': True,
            'video_path': video_path,
            'output_path': output_path,
            'main_exercise': main_exercise,
            'exercise_detections': exercise_detections,
            'stats': stats,
            'success_rate': success_rate,
            'frame_results': frame_results,
            'total_frames_analyzed': len(frame_results),
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def run_realtime_analysis(self, camera_id: int = 0, manual_exercise: str = None):
        """ğŸ® ì‹¤ì‹œê°„ ì™„ì „ ìë™ ë¶„ì„"""
        cap = cv2.VideoCapture(camera_id)
        if not cap.isOpened():
            print(f"âŒ ì¹´ë©”ë¼ {camera_id} ì—´ê¸° ì‹¤íŒ¨")
            return False
        
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)

        cv2.namedWindow('Exercise Analysis', cv2.WINDOW_NORMAL)
        cv2.resizeWindow('Exercise Analysis', 1200, 800)
        
        print("\n" + "="*80)
        print("ğŸ¤– ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ ì‹œìŠ¤í…œ")
        print("="*80)
        print("âœ¨ ê¸°ëŠ¥:")
        print("  ğŸ¤– 1ë‹¨ê³„: AIê°€ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€")
        print("  ğŸ¯ 2ë‹¨ê³„: ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ì •ë°€ ê°ë„ ë¶„ì„")
        print("  ğŸ—£ï¸ 3ë‹¨ê³„: ìš´ë™ë³„ ë§ì¶¤ ìƒì„¸ í”¼ë“œë°±")
        print("  ğŸŒˆ 4ë‹¨ê³„: ì‹¤ì‹œê°„ ì´ˆë¡/ë¹¨ê°• í™”ë©´ + í…Œë‘ë¦¬")
        print("  ğŸ“Š 5ë‹¨ê³„: ì‹¤ì‹œê°„ í†µê³„ ë° ì„±ê³¼ ì¶”ì ")
        print("\nğŸ“ í™”ë©´ êµ¬ì„±:")
        print("  â€¢ ì™¼ìª½ ìœ„: ê°ì§€ëœ ìš´ë™ ì¢…ë¥˜")
        print("  â€¢ ì™¼ìª½ ì•„ë˜: ìƒì„¸ í”¼ë“œë°± ë©”ì‹œì§€")
        print("  â€¢ ì˜¤ë¥¸ìª½ ìœ„: ìš´ë™ í†µê³„")
        print("  â€¢ ì¤‘ì•™: ìì„¸ ìƒíƒœ (Good/Bad)")
        print("  â€¢ ì „ì²´: ì´ˆë¡/ë¹¨ê°• í…Œë‘ë¦¬ + ë°°ê²½")
        print("\nâŒ¨ï¸ ì¡°ì‘ë²•:")
        print("  Q: ì¢…ë£Œ | R: í†µê³„ ë¦¬ì…‹ | S: ìŠ¤í¬ë¦°ìƒ·")
        print("  C: ìˆ˜ë™ ìš´ë™ ì„ íƒ | SPACE: ìë™/ìˆ˜ë™ ëª¨ë“œ í† ê¸€")
        print("="*80)
        
        # ëª¨ë¸ ìƒíƒœ í™•ì¸
        if not self.model_loaded:
            print("âš ï¸ AI ëª¨ë¸ ì—†ìŒ - ìˆ˜ë™ ëª¨ë“œë¡œ ì‹œì‘")
            if manual_exercise:
                self.current_exercise = manual_exercise
                exercise_info = self.exercise_info.get(manual_exercise, {})
                print(f"ìˆ˜ë™ ì„ íƒ: {exercise_info.get('emoji', 'ğŸ‹ï¸')} {exercise_info.get('name_kr', manual_exercise)}")
        
        # ìˆ˜ë™ ìš´ë™ ì„ íƒìš©
        available_exercises = list(self.exercise_thresholds.keys())
        manual_mode = not self.model_loaded
        current_manual_idx = 0
        
        if manual_exercise and manual_exercise in available_exercises:
            current_manual_idx = available_exercises.index(manual_exercise)
            self.current_exercise = manual_exercise
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame = cv2.flip(frame, 1)  # ì…€ì¹´ ëª¨ë“œ
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = self.pose_video.process(frame_rgb)
                
                if results.pose_landmarks:
                    # ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
                    self.mp_drawing.draw_landmarks(
                        frame, results.pose_landmarks, self.mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style())
                    
                    # ğŸ¤– 1ë‹¨ê³„: AI ìš´ë™ ê°ì§€ (ìë™ ëª¨ë“œì¼ ë•Œë§Œ)
                    if not manual_mode and self.model_loaded:
                        exercise, confidence = self.classify_exercise(frame)
                    else:
                        exercise = self.current_exercise
                        confidence = 1.0
                    
                    # ğŸ¯ 2ë‹¨ê³„: ê°ë„ ë¶„ì„
                    if exercise in self.exercise_thresholds:
                        pose_result = self.analyze_pose_angles(results.pose_landmarks.landmark, exercise)
                        
                        if pose_result['valid']:
                            # í†µê³„ ì—…ë°ì´íŠ¸
                            self.stats['frames'] += 1
                            pose_quality = pose_result['classification']
                            self.stats[pose_quality] += 1
                            
                            # âœ¨ 3-4ë‹¨ê³„: í”¼ë“œë°± + í™”ë©´ ì˜¤ë²„ë ˆì´
                            frame = self.draw_enhanced_overlay(frame, exercise, pose_result)
                        else:
                            frame = self.draw_enhanced_overlay(frame, exercise, {'valid': False})
                    else:
                        frame = self.draw_enhanced_overlay(frame, exercise, {'valid': False})
                else:
                    # í¬ì¦ˆ ë¯¸ê°ì§€
                    cv2.rectangle(frame, (0, 0), (frame.shape[1], frame.shape[0]), (255, 255, 0), 30)
                    message = "ì „ì‹ ì´ ë³´ì´ë„ë¡ ì¹´ë©”ë¼ ì•ì— ì„œì£¼ì„¸ìš”"
                    text_size = cv2.getTextSize(message, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]
                    text_x = (frame.shape[1] - text_size[0]) // 2
                    text_y = frame.shape[0] // 2
                    
                    cv2.rectangle(frame, (text_x - 20, text_y - 40), 
                                 (text_x + text_size[0] + 20, text_y + 10), (0, 0, 0), -1)
                    cv2.putText(frame, message, (text_x, text_y), 
                               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
                
                # í™”ë©´ ì¶œë ¥
                window_title = "ğŸ¤– ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ ì‹œìŠ¤í…œ"
                cv2.imshow(window_title, frame)
                
                # í‚¤ ì…ë ¥ ì²˜ë¦¬
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('r'):
                    # í†µê³„ ë¦¬ì…‹
                    self.stats = {'good': 0, 'bad': 0, 'frames': 0}
                    self.exercise_history.clear()
                    self.pose_history.clear()
                    print("ğŸ“Š í†µê³„ ë¦¬ì…‹ ì™„ë£Œ")
                elif key == ord('s'):
                    # ìŠ¤í¬ë¦°ìƒ·
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"complete_analysis_screenshot_{timestamp}.jpg"
                    cv2.imwrite(filename, frame)
                    print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filename}")
                elif key == ord('c'):
                    # ìˆ˜ë™ ìš´ë™ ë³€ê²½
                    current_manual_idx = (current_manual_idx + 1) % len(available_exercises)
                    self.current_exercise = available_exercises[current_manual_idx]
                    exercise_info = self.exercise_info.get(self.current_exercise, {})
                    emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                    name_kr = exercise_info.get('name_kr', self.current_exercise)
                    print(f"ğŸ”„ ìˆ˜ë™ ì„ íƒ: {emoji} {name_kr}")
                elif key == ord(' '):
                    # ìë™/ìˆ˜ë™ ëª¨ë“œ í† ê¸€
                    manual_mode = not manual_mode
                    mode = "ìˆ˜ë™" if manual_mode else "ìë™"
                    print(f"ğŸ”„ {mode} ëª¨ë“œë¡œ ë³€ê²½")
        
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ìµœì¢… í†µê³„
            total = self.stats['good'] + self.stats['bad']
            if total > 0:
                success_rate = (self.stats['good'] / total) * 100
                print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
                print(f"  ğŸ¯ ì´ ë¶„ì„: {total} í”„ë ˆì„")
                print(f"  âœ… Good: {self.stats['good']} ({success_rate:.1f}%)")
                print(f"  âŒ Bad: {self.stats['bad']} ({100-success_rate:.1f}%)")
                print(f"  ğŸ¯ ìš´ë™ë³„ ë¶„ì„ ì™„ë£Œ!")
            
            return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ğŸ¤– ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ê¸° - ì‚¬ì§„/ì˜ìƒ/ì‹¤ì‹œê°„ í†µí•©',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ¯ ì™„ì „ ìë™í™” ê¸°ëŠ¥:
  1ë‹¨ê³„: ğŸ¤– AIê°€ ìš´ë™ ì¢…ë¥˜ ìë™ ê°ì§€ (ìŠ¤ì¿¼íŠ¸, í‘¸ì‰¬ì—…, ë°ë“œë¦¬í”„íŠ¸, ë²¤ì¹˜í”„ë ˆìŠ¤, ëŸ°ì§€)
  2ë‹¨ê³„: ğŸ¯ ê°ì§€ëœ ìš´ë™ì— ë§ì¶° ì •ë°€ ê°ë„ ë¶„ì„ 
  3ë‹¨ê³„: ğŸ—£ï¸ ìš´ë™ë³„ ë§ì¶¤ ìƒì„¸ í”¼ë“œë°±
  4ë‹¨ê³„: ğŸŒˆ ì‹¤ì‹œê°„ ì´ˆë¡/ë¹¨ê°• í™”ë©´ + í…Œë‘ë¦¬
  5ë‹¨ê³„: ğŸ“Š ì‹¤ì‹œê°„ í†µê³„ ë° ì„±ê³¼ ì¶”ì 

ğŸ“ í™”ë©´ êµ¬ì„±:
  â€¢ ì™¼ìª½ ìœ„: ê°ì§€ëœ ìš´ë™ ì¢…ë¥˜ + ì‹ ë¢°ë„
  â€¢ ì™¼ìª½ ì•„ë˜: ìƒì„¸ í”¼ë“œë°± ë©”ì‹œì§€ (ê°ë„ë³„ ì¡°ì–¸)
  â€¢ ì˜¤ë¥¸ìª½ ìœ„: ìš´ë™ í†µê³„ (Good/Bad ë¹„ìœ¨)
  â€¢ ì¤‘ì•™: ìì„¸ ìƒíƒœ (ì™„ë²½í•œ ìì„¸! / ìì„¸ êµì • í•„ìš”)
  â€¢ ì „ì²´: ì´ˆë¡(Good)/ë¹¨ê°•(Bad) í…Œë‘ë¦¬ + ë°°ê²½


ğŸ¯ ì‚¬ìš© ì˜ˆì‹œ:
  # ì‹¤ì‹œê°„ ì™„ì „ ìë™ ë¶„ì„
  python complete_auto_analyzer.py --mode realtime
  
  # ì‹¤ì‹œê°„ + ìˆ˜ë™ ìš´ë™ ì§€ì •
  python complete_auto_analyzer.py --mode realtime --manual squat
  
  # ì‚¬ì§„ ì™„ì „ ìë™ ë¶„ì„ 
  python complete_auto_analyzer.py --mode image --input photo.jpg
  
  # ì˜ìƒ ì™„ì „ ìë™ ë¶„ì„
  python complete_auto_analyzer.py --mode video --input video.mp4 --output analyzed.mp4

âŒ¨ï¸ ì‹¤ì‹œê°„ ì¡°ì‘:
  Q: ì¢…ë£Œ  |  R: í†µê³„ ë¦¬ì…‹  |  S: ìŠ¤í¬ë¦°ìƒ·
  C: ìˆ˜ë™ ìš´ë™ ë³€ê²½  |  SPACE: ìë™/ìˆ˜ë™ ëª¨ë“œ í† ê¸€

ğŸ‹ï¸ ì§€ì› ìš´ë™ & ìƒì„¸ í”¼ë“œë°±:
  ğŸ‹ï¸â€â™€ï¸ ìŠ¤ì¿¼íŠ¸: ë¬´ë¦/ì—‰ë©ì´ ê°ë„, ë“± ê³§ê²Œ í´ê¸°, ë°œë ë„˜ì§€ ì•Šê¸°
  ğŸ’ª í‘¸ì‰¬ì—…: íŒ”ê¿ˆì¹˜ ê°ë„, ëª¸ ì¼ì§ì„ , ì–´ê¹¨ ì•ˆì •ì„±
  ğŸ‹ï¸â€â™‚ï¸ ë°ë“œë¦¬í”„íŠ¸: í™ íŒì§€, ë“± ê³§ê²Œ, ë¬´ë¦ ê°ë„ (ì™„í™” ê¸°ì¤€)
  ğŸ”¥ ë²¤ì¹˜í”„ë ˆìŠ¤: íŒ”ê¿ˆì¹˜/ì–´ê¹¨ ê°ë„, ë“± ì•„ì¹˜
  ğŸš€ ëŸ°ì§€: ì•ë¬´ë¦ 90ë„, ë’·ë¬´ë¦ í´ê¸°, ìƒì²´ ì§ë¦½

ğŸ’¡ AI ëª¨ë¸ í•„ìš”:
  models/exercise_classifier.pkl íŒŒì¼ì´ ìˆìœ¼ë©´ ì™„ì „ ìë™
  ì—†ìœ¼ë©´ ìˆ˜ë™ ìš´ë™ ì„ íƒ ëª¨ë“œë¡œ ë™ì‘
        """
    )
    
    parser.add_argument('--mode', type=str, default='realtime',
                       choices=['realtime', 'image', 'video'],
                       help='ë¶„ì„ ëª¨ë“œ: realtime(ì‹¤ì‹œê°„), image(ì‚¬ì§„), video(ì˜ìƒ)')
    parser.add_argument('--input', type=str,
                       help='ì…ë ¥ íŒŒì¼ ê²½ë¡œ (image/video ëª¨ë“œìš©)')
    parser.add_argument('--output', type=str,
                       help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (video ëª¨ë“œìš©)')
    parser.add_argument('--camera', type=int, default=0,
                       help='ì¹´ë©”ë¼ ID (realtime ëª¨ë“œìš©)')
    parser.add_argument('--manual', type=str,
                       choices=['squat', 'push_up', 'deadlift', 'bench_press', 'lunge'],
                       help='ìˆ˜ë™ ìš´ë™ ì„ íƒ (AI ê°ì§€ ê±´ë„ˆë›°ê¸°)')
    
    args = parser.parse_args()
    
    # ì™„ì „ ìë™í™” ë¶„ì„ê¸° ì´ˆê¸°í™”
    try:
        analyzer = CompleteAutoExerciseAnalyzer()
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return 1
    
    print("ğŸ¤– ì™„ì „ ìë™í™” ìš´ë™ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘!")
    print("="*80)
    print("ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:")
    print("  ğŸ¤– AI ìë™ ìš´ë™ ê°ì§€ (5ì¢…ëª©)")
    print("  ğŸ“ ì •ë°€ ê°ë„ ë¶„ì„")
    print("  ğŸ—£ï¸ ìš´ë™ë³„ ë§ì¶¤ ìƒì„¸ í”¼ë“œë°±")
    print("  ğŸŒˆ ì‹¤ì‹œê°„ ì´ˆë¡/ë¹¨ê°• í”¼ë“œë°±")
    print("  ğŸ“Š ì„±ê³¼ ì¶”ì ")
    print("  ğŸ“· ì‚¬ì§„/ğŸ¬ ì˜ìƒ/ğŸ® ì‹¤ì‹œê°„ ëª¨ë‘ ì§€ì›")
    
    try:
        if args.mode == 'realtime':
            print(f"\nğŸ® ì‹¤ì‹œê°„ ë¶„ì„ ì‹œì‘ (ì¹´ë©”ë¼ {args.camera})")
            if args.manual:
                exercise_info = analyzer.exercise_info.get(args.manual, {})
                emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                name_kr = exercise_info.get('name_kr', args.manual)
                print(f"ğŸ”§ ìˆ˜ë™ ëª¨ë“œ: {emoji} {name_kr}")
            success = analyzer.run_realtime_analysis(args.camera, args.manual)
            return 0 if success else 1
            
        elif args.mode == 'image':
            if not args.input:
                print("âŒ --input ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤ (ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ)")
                return 1
            
            print(f"\nğŸ“· ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {args.input}")
            result = analyzer.analyze_single_image(args.input)
            
            if result.get('success', False):
                # ê²°ê³¼ ì¶œë ¥
                exercise_info = result['exercise_info']
                emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                name_kr = exercise_info.get('name_kr', 'unknown')
                exercise_conf = result['exercise_confidence']
                pose_result = result['pose_analysis']
                
                print(f"\nğŸ‰ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ!")
                print(f"ğŸ¤– AI ê°ì§€: {emoji} {name_kr} (ì‹ ë¢°ë„: {exercise_conf:.1%})")
                
                if pose_result['valid']:
                    pose_quality = pose_result['classification']
                    pose_conf = pose_result['confidence']
                    
                    status_emoji = "âœ…" if pose_quality == 'good' else "âš ï¸"
                    print(f"ğŸ¯ ìì„¸ ë¶„ì„: {status_emoji} {pose_quality.upper()} (ì ìˆ˜: {pose_conf:.1%})")
                    
                    # í”¼ë“œë°± ë©”ì‹œì§€ ì¶œë ¥
                    feedback_messages = result['feedback_messages']
                    if feedback_messages:
                        print(f"\nğŸ’¬ ìƒì„¸ í”¼ë“œë°±:")
                        for i, message in enumerate(feedback_messages[:5], 1):
                            print(f"  {i}. {message}")
                    
                    # ìœ„ë°˜ì‚¬í•­ ì¶œë ¥
                    violations = pose_result.get('violations', [])
                    if violations:
                        print(f"\nğŸ“ ê°ë„ ë¶„ì„:")
                        for violation in violations[:3]:
                            joint_kr = violation.get('name_kr', violation['joint'])
                            angle = violation['angle']
                            range_min, range_max = violation['expected_range']
                            print(f"  â€¢ {joint_kr}: {angle:.1f}Â° â†’ ëª©í‘œ: {range_min:.0f}-{range_max:.0f}Â°")
                
                # ì£¼ì„ ì´ë¯¸ì§€ í‘œì‹œ
                annotated_image = result['annotated_image']
                
                # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (í™”ë©´ì— ë§ê²Œ)
                height, width = annotated_image.shape[:2]
                if width > 1200:
                    scale = 1200 / width
                    new_width = int(width * scale)
                    new_height = int(height * scale)
                    annotated_image = cv2.resize(annotated_image, (new_width, new_height))
                
                window_title = f"ì™„ì „ ìë™ ë¶„ì„ ê²°ê³¼: {emoji} {name_kr}"
                cv2.imshow(window_title, annotated_image)
                
                print(f"\nğŸ–¼ï¸ ë¶„ì„ ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ ì¤‘... (ì•„ë¬´ í‚¤ë‚˜ ëˆŒëŸ¬ì„œ ë‹«ê¸°)")
                cv2.waitKey(0)
                cv2.destroyAllWindows()
                
            else:
                print(f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return 1
                
        elif args.mode == 'video':
            if not args.input:
                print("âŒ --input ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤ (ì˜ìƒ íŒŒì¼ ê²½ë¡œ)")
                return 1
            
            print(f"\nğŸ¬ ì˜ìƒ ë¶„ì„ ì‹œì‘: {args.input}")
            if args.output:
                print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {args.output}")
            
            result = analyzer.analyze_video_file(args.input, args.output)
            
            if result.get('success', False):
                # ê²°ê³¼ ì¶œë ¥
                main_exercise = result['main_exercise']
                exercise_info = analyzer.exercise_info.get(main_exercise, {})
                emoji = exercise_info.get('emoji', 'ğŸ‹ï¸')
                name_kr = exercise_info.get('name_kr', main_exercise)
                
                stats = result['stats']
                success_rate = result['success_rate']
                total_analyzed = result['total_frames_analyzed']
                
                print(f"\nğŸ‰ ì˜ìƒ ë¶„ì„ ì™„ë£Œ!")
                print(f"ğŸ¯ ì£¼ìš” ìš´ë™: {emoji} {name_kr}")
                print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
                print(f"  â€¢ ì´ ë¶„ì„ í”„ë ˆì„: {total_analyzed}ê°œ")
                print(f"  â€¢ âœ… Good ìì„¸: {stats['good']}í”„ë ˆì„")
                print(f"  â€¢ âŒ Bad ìì„¸: {stats['bad']}í”„ë ˆì„")
                print(f"  â€¢ ğŸ¯ ì„±ê³µë¥ : {success_rate:.1f}%")
                
                # ìš´ë™ ê°ì§€ í†µê³„
                exercise_detections = result['exercise_detections']
                if len(exercise_detections) > 1:
                    print(f"\nğŸ“ˆ ìš´ë™ ê°ì§€ í†µê³„:")
                    for exercise, count in exercise_detections.items():
                        info = analyzer.exercise_info.get(exercise, {})
                        emoji = info.get('emoji', 'ğŸ‹ï¸')
                        name_kr = info.get('name_kr', exercise)
                        percentage = (count / sum(exercise_detections.values())) * 100
                        print(f"  â€¢ {emoji} {name_kr}: {count}í”„ë ˆì„ ({percentage:.1f}%)")
                
                if args.output:
                    print(f"\nğŸ’¾ ì£¼ì„ ì˜ìƒ ì €ì¥: {args.output}")
                
            else:
                print(f"âŒ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return 1
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
        return 0
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())